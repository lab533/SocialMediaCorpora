{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GTX 690 (CNMeM is disabled, cuDNN not available)\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import numpy as np\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from  nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import scipy as sc\n",
    "from sklearn.metrics import mutual_info_score\n",
    "\n",
    "stops = set(stopwords.words(\"russian\"))\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "news_model = Word2Vec()\n",
    "news_model = Word2Vec().load_word2vec_format(\"news/news_sg_w2v.bin\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9285714285714286"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_phrases(path):\n",
    "    str_xml = open(path, 'r', encoding=\"utf8\").read()\n",
    "    root = ET.fromstring(str_xml)\n",
    "    sentences = {}\n",
    "    pairs = {}    \n",
    "    for child in root.find(\"corpus\"):\n",
    "        parafrase = {}    \n",
    "        for value in child:\n",
    "            if value.get(\"name\") == \"id_1\":\n",
    "                parafrase[\"id_1\"] = int(value.text)            \n",
    "            elif value.get(\"name\") == \"id_2\":\n",
    "                parafrase[\"id_2\"] = int(value.text)\n",
    "            elif value.get(\"name\") == \"text_1\":\n",
    "                parafrase[\"text_1\"] = value.text\n",
    "            elif value.get(\"name\") == \"text_2\":\n",
    "                parafrase[\"text_2\"] = value.text\n",
    "            elif value.get(\"name\") == \"class\":\n",
    "                parafrase[\"class\"] = int(value.text)\n",
    "            elif value.get(\"name\") == \"jaccard\":\n",
    "                parafrase[\"jaccard\"] = float(value.text)\n",
    "            elif value.get(\"name\") == \"id\":\n",
    "                parafrase[\"id\"] = int(value.text)          \n",
    "        sentences[parafrase[\"id_1\"]] = tokenizer.tokenize(parafrase[\"text_1\"].lower())\n",
    "        sentences[parafrase[\"id_2\"]] = tokenizer.tokenize(parafrase[\"text_2\"].lower())\n",
    "        pairs[parafrase[\"id\"]] = {\"sents\": (parafrase[\"id_1\"], parafrase[\"id_2\"]),\n",
    "                                  \"class\": parafrase.get(\"class\"),\n",
    "                                  \"jaccard\": parafrase.get(\"jaccard\")}\n",
    "    return sentences, pairs    \n",
    "\n",
    "\n",
    "def write_sentences(sentences, path):\n",
    "    with open(path, 'w', encoding=\"utf8\") as f:\n",
    "        for item in sentences.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(item[0], item[1].replace(\"\\n\", \"\").replace(\"\\t\", \"\")))\n",
    "            \n",
    "            \n",
    "def read_parsed_sentences(path):\n",
    "    parsed = {}\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            id_sentence = line.split('\\t')\n",
    "            s_id = int(id_sentence[0].split('_')[-1])\n",
    "            id_sentence[1] = id_sentence[1].replace('\\n',\"\")\n",
    "            if s_id not in parsed:\n",
    "                parsed[s_id] =[token.split('_')[0] for token in id_sentence[1].lower().split(' ')]\n",
    "                # parsed[s_id] = id_sentence[1].lower().split(' ')\n",
    "#             elif  \" \".join(parsed[s_id]) !=  id_sentence[1].lower():\n",
    "#                 print(\"same id {}, different texts!!!\\n{}\\n{}\\n\".format(s_id, parsed[s_id], id_sentence[1]))\n",
    "    return parsed \n",
    "\n",
    "\n",
    "def read_parsed_sentences2(path):\n",
    "    parsed = {}\n",
    "    with open(path, 'r', encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "            id_sentence = line.split('\\t')\n",
    "            s_id = int(id_sentence[0].replace('\\ufeff', \"\"))\n",
    "            # id_sentence[1] = id_sentence[1].replace('\\n',\"\")\n",
    "            if s_id not in parsed:\n",
    "                parsed[s_id] =[token for token in id_sentence[1].lower().split(' ')]              \n",
    "    return parsed \n",
    "\n",
    "\n",
    "def contains_num(sentence):\n",
    "    # tokens = tokenizer.tokenize(sentence)\n",
    "    for token in sentence:\n",
    "        if str.isnumeric(token):\n",
    "            return token\n",
    "        \n",
    "        \n",
    "def get_nums_similiarity(first_nums, second_nums, eps=10**(-5)):\n",
    "    sims = []\n",
    "    if len(first_nums) == 0 or len(second_nums) == 0:\n",
    "        return 0.0\n",
    "    for first in first_nums:\n",
    "        most_similar = 0\n",
    "        for second in second_nums:\n",
    "            if max(first, second) != 0 and most_similar < min(first, second)*1./max(first, second):\n",
    "                most_similar = min(first, second)*1./max(first, second)               \n",
    "            elif max(first, second) - min(first, second) < eps:\n",
    "                most_similar = 1\n",
    "        sims.append(most_similar)\n",
    "    return np.sum(sims)*1./max(len(first_nums), len(second_nums))\n",
    "\n",
    "\n",
    "def get_pair_features(first_sent, second_sent, word2vec, ldamodel, dictionary, stops=set(),\n",
    "                     sent_topics=None, first_id=None, second_id=None):\n",
    "    features = {}\n",
    "    one_no =int((first_sent.count(\"не\") > 0) != (second_sent.count(\"не\") > 0))\n",
    "    first = [token for token in first_sent if token not in stops]\n",
    "    second = [token for token in second_sent if token not in stops]\n",
    "    jaccard = get_jaccard(first, second)\n",
    "    first_nums = extract_nums(first)\n",
    "    second_nums = extract_nums(second)\n",
    "    num_sims = get_nums_similiarity(first_nums, second_nums)\n",
    "    contains_numeric = int( (len(first_nums) + len(second_nums)) > 0)\n",
    "    # print( (first, second))\n",
    "    w2v_similiarity = get_vecs_similiarity(build_sent_matrix(first, word2vec),\n",
    "                                          build_sent_matrix(second, word2vec))\n",
    "    features = {\"не_xor\":one_no,\n",
    "                \"jaccard\": jaccard,                \n",
    "                \"contains_num\": contains_numeric,\n",
    "                \"nums_sim\": num_sims,\n",
    "                \"word2vec_sim\": w2v_similiarity\n",
    "               }\n",
    "    f_bow = dictionary.doc2bow(first)\n",
    "    s_bow = dictionary.doc2bow(second)\n",
    "    f_topic = np.zeros(70, dtype=np.float)\n",
    "    s_topic = np.zeros(70, dtype=np.float)    \n",
    "    if sent_topics != None:\n",
    "        f_topic = sent_topics[first_id]\n",
    "        s_topic = sent_topics[second_id]\n",
    "    else:    \n",
    "        for topic, prob in ldamodel.get_document_topics(f_bow, minimum_probability=0.005):\n",
    "            f_topic[topic] = prob        \n",
    "        for topic, prob in ldamodel.get_document_topics(s_bow, minimum_probability=0.005):\n",
    "            s_topic[topic] = prob    \n",
    "    cross = get_cross_entropy(f_topic, s_topic)\n",
    "    \n",
    "    features = {\"не_xor\":one_no,\n",
    "                \"jaccard\": jaccard,                \n",
    "                \"contains_num\": contains_numeric,\n",
    "                \"nums_sim\": num_sims,\n",
    "                \"word2vec_sim\": w2v_similiarity,\n",
    "                \"cross\": cross\n",
    "               }\n",
    "    return features, [w2v_similiarity, jaccard,  one_no, num_sims, contains_numeric, cross]\n",
    "\n",
    "\n",
    "def extract_nums(sentence):\n",
    "    return [float(token) for token in sentence if str.isnumeric(token)]\n",
    "\n",
    "\n",
    "def build_sent_matrix(sentence, word2vec):\n",
    "    return np.array([word2vec[token] for token in sentence if token in word2vec])\n",
    "    \n",
    "\n",
    "def get_vecs_similiarity(first_vecs, second_vecs):\n",
    "    # first_vecs = build_sent_matrix()    \n",
    "    distances = ((first_vecs.T/np.linalg.norm(first_vecs, axis=1)).T).dot((second_vecs.T/np.linalg.norm(second_vecs, axis=1)))\n",
    "    if distances.shape[0] < distances.shape[1]:\n",
    "        return distances.max(axis=1).mean()\n",
    "    else:\n",
    "        return distances.max(axis=0).mean()\n",
    "    # first_sims = distances.max(axis=1)\n",
    "    # second_sims = distances.max(axis=0)\n",
    "    # return get_nums_similiarity(first_sims, second_sims)\n",
    "\n",
    "\n",
    "def get_jaccard(first, second):\n",
    "    return len(set(first).intersection(set(second))) * 1. /len(set(first).union(set(second)))\n",
    "    \n",
    "\n",
    "def get_cross_entropy(first_vec, second_vec, min_p=0.01):\n",
    "    # mask = (first_vec > min_p)*(second_vec > min_p)\n",
    "    # mutual_info_score()\n",
    "    return first_vec.dot(second_vec).sum()\n",
    "\n",
    "    \n",
    "get_nums_similiarity([1,3],[1,3.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = \"paraphraser/paraphrases.xml\"\n",
    "test_path = \"paraphraser/paraphrases_test.xml\"\n",
    "train_sent_path = \"paraphraser/train_sentences.tsv\"\n",
    "test_sent_path = \"paraphraser/test_sentences.tsv\"\n",
    "parsed_sentences_path = \"parsed/sentences.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences, train_pairs = read_phrases(train_path)\n",
    "# write_sentences(sentences, train_sent_path)\n",
    "test_sentences, test_pairs = read_phrases(test_path)\n",
    "# write_sentences(sentences, test_sent_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parsed_sentences = read_parsed_sentences(parsed_sentences_path)\n",
    "parsed2 = read_parsed_sentences2(\"paraphraser/res_all.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dictionary = corpora.Dictionary.load(\"ldamodel/dictionary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#ldamodel = models.ldamodel.LdaModel(id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n"
     ]
    }
   ],
   "source": [
    "# ldamodel = ldamodel.load('ldamodel/lda')\n",
    "sent_topics = {}\n",
    "count = 0\n",
    "for key in parsed2.keys():  \n",
    "    if key not in sent_topics:\n",
    "        topic = np.zeros(70, dtype=np.float)\n",
    "        for t, prob in ldamodel.get_document_topics(dictionary.doc2bow(parsed2[key]), minimum_probability=0.005):\n",
    "            topic[t] = prob\n",
    "        sent_topics[key] = topic\n",
    "    count += 1   \n",
    "    if count % 500 == 0:\n",
    "        print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = []\n",
    "y_tr = []\n",
    "counts = np.zeros(3)\n",
    "\n",
    "\n",
    "count = 0\n",
    "pairs = train_pairs\n",
    "sentences = parsed2#_sentences\n",
    "for item in train_pairs.items():\n",
    "    # counts[item[1][\"class\"] + 1] += 1\n",
    "    first_id = item[1]['sents'][0]\n",
    "    second_id = item[1]['sents'][1]\n",
    "    features, x = get_pair_features(parsed2[first_id],parsed2[second_id], news_model, ldamodel, dictionary,\n",
    "                                   set([]), sent_topics, first_id, second_id)\n",
    "    X_tr.append(x)\n",
    "    y_tr.append(item[1][\"class\"])\n",
    "    count += 1\n",
    "    # if count % 500: \n",
    "    #    print(\"count: {}\".format(count))\n",
    "X_tr = np.array(X_tr)\n",
    "y_tr = np.array(y_tr)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_te = []\n",
    "y_te = []\n",
    "test_ids = []\n",
    "for item in test_pairs.items():\n",
    "    # counts[item[1][\"class\"] + 1] += 1\n",
    "    first_id = item[1]['sents'][0]\n",
    "    second_id = item[1]['sents'][1]\n",
    "    features, x = get_pair_features(parsed2[first_id],parsed2[second_id], news_model, ldamodel, dictionary,\n",
    "                                   set([]), sent_topics, first_id, second_id)    \n",
    "    X_te.append(x)\n",
    "    if item[1][\"class\"] != None:\n",
    "        y_te.append(item[1][\"class\"])\n",
    "    else:\n",
    "        y_te.append(-2)\n",
    "    test_ids.append(item[0])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#   if features[\"jaccard\"] < 0.3 :#contains_num(sentences[first_id]) or contains_num(sentences[second_id]):\n",
    "#         N += 1\n",
    "#         print(\"class: {}\\nfeatures: {}\\n{}\\n{}\\n\".format(y[-1], \n",
    "#                                                         features,\n",
    "#                                                         \" \".join(sentences[first_id]),\n",
    "#                                                         \" \".join(sentences[second_id])))\n",
    "#         print(\"{}\\n{}\\n\".format(\" \".join(parsed_sentences[first_id]),\n",
    "#                                 \" \".join(parsed_sentences[second_id])))\n",
    "        \n",
    "X_te = np.array(X_te)\n",
    "y_te = np.array(y_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytr2 = np.ones_like(y_tr, dtype=np.int)\n",
    "ytr2[y_tr < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'C': [1, 10, 100, 1000], 'kernel': ['linear']}, {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_svm= [\n",
    "  {'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n",
    "  {'C': [1, 10, 100, 1000], 'gamma': [0.001, 0.0001], 'kernel': ['rbf']},\n",
    " ]\n",
    "svr = svm.SVC()\n",
    "gs_svr = GridSearchCV(svr, param_svm)\n",
    "gs_svr.fit(X_tr[:, mask], y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59526774595267751"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_svr.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_grid = [\n",
    "  {'max_features': [2, 3, 7 ],\n",
    "   'max_depth': [2, 5, 15],\n",
    "   }\n",
    " ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=None, error_score='raise',\n",
       "       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=200, n_jobs=1, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid=[{'n_estimators': [100, 200], 'max_depth': [2, 5, 10], 'max_features': [2, 3, 5]}],\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf = GridSearchCV(RandomForestClassifier(n_estimators=200), param_grid)\n",
    "grid_clf.fit(X_tr[:,mask], y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59250034592500345"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_clf.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.56340298099087494"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w2v_similiarity, jaccard,  one_no, num_sims, contains_numeric, cross\n",
    "mask = np.array([0,1,2,3,5])\n",
    "clf = RandomForestClassifier(n_estimators=100)\n",
    "scores = cross_val_score(clf, X_tr[:, mask], y_tr, cv=10)\n",
    "scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.61904761904761907"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = grid_clf.best_estimator_.predict(X_te[:, mask])\n",
    "# res = gs_svr.best_estimator_.predict(X_te[:, mask])\n",
    "# grid_clf.best_score_\n",
    "np.mean(y_te[y_te > -2] == res[y_te > -2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('res2svm.tsv', 'w') as f:\n",
    "    for cid, res in zip(test_ids, res):\n",
    "        f.write(str(cid) + \"\\t\" + str(res) + \"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Извраты с Word2Vec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Phrases, Word2Vec\n",
    "\n",
    "class LowerSentences(object):\n",
    "    def __init__(self, fnames):\n",
    "        self.fnames = fnames\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in self.fnames:\n",
    "            for line in open(fname):\n",
    "                yield line.split()\n",
    "                \n",
    "                \n",
    "def rewrite_without_pos(fin, fout):\n",
    "    with open(fin, 'r') as src, open(fout, 'w') as dst:\n",
    "        for line in src:\n",
    "            tokens = [token.split('_')[0] for token in line.lower().split()]\n",
    "            dst.write(\" \".join([token for token in tokens if str.isalpha(token)]))\n",
    "            dst.write('\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_news = \"/home/user1/projects/Paraphrase/news/RDOC_lower.txt\"\n",
    "sentences_LJ = \"/home/user1/projects/Paraphrase/LJ/LJ_lower_28082016.txt\" \n",
    "# rewrite_without_pos(\"../../ipython_notebooks/Word2VecUtils/lemmatized_texts_28082016.txt\",\"LJ/LJ_lower_28082016.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stream = LowerSentences([sentences_news])\n",
    "news_bigram = Phrases(news_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "LJ_stream = LowerSentences([sentences_LJ])\n",
    "LJ_bigram = Phrases(LJ_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 10\n",
    "# for item in news_model.vocab:\n",
    "#     print(item)\n",
    "#     count -=1\n",
    "#     if count == 0:\n",
    "#         break\n",
    "# news_model.similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TOPIC MODELING\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora\n",
    "from gensim import models\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class LowerSentencesWithoutStops(object):\n",
    "    def __init__(self, fnames, stops):\n",
    "        self.fnames = fnames\n",
    " \n",
    "    def __iter__(self):\n",
    "        for fname in self.fnames:\n",
    "            for line in open(fname):\n",
    "                yield [token for token in line.split() if token not in stops]\n",
    "                \n",
    "                \n",
    "            \n",
    "stops = set(stopwords.words(\"russian\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "news_stream_f = LowerSentencesWithoutStops([sentences_news], stops)\n",
    "dictionary = corpora.Dictionary(news_stream_f)   # составляем словарь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_stream = LowerSentences([sentences_news])\n",
    "corpus = [dictionary.doc2bow(sent) for sent in news_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary = corpora.Dictionary([[\"a\", \"b\"]])\n",
    "# dictionary.doc2bow([\"a\", \"b\", \"c\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(392887, 'оставшеося')\n",
      "(77948, 'maxim')\n",
      "(1066203, 'хэдоу')\n",
      "(235963, 'всестороной')\n",
      "(231222, 'посапывать')\n",
      "(302087, 'herold')\n",
      "(805234, 'гильотно')\n",
      "(604261, 'старокалужский')\n",
      "(538110, 'магистрал')\n",
      "(465908, 'коломойца')\n"
     ]
    }
   ],
   "source": [
    "count = 10\n",
    "for item in dictionary.items():\n",
    "    print(item)\n",
    "    count -= 1\n",
    "    if count == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.id2token[0]\n",
    "# freqs = []\n",
    "low_keys = []\n",
    "count=100\n",
    "for key, fr in dictionary.dfs.items():\n",
    "    freqs.append(fr)\n",
    "    if (fr > 250000) or (fr < 35):        \n",
    "        low_keys.append(key)    \n",
    "        # print(dictionary.id2token[key])\n",
    "        # count -= 1\n",
    "    if count == 0:    \n",
    "        break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(low_keys)\n",
    "dictionary.filter_tokens(bad_ids=low_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1MqAkCS1MiAkSa0MCElSKwNCktTq/wNu5BGIRf1lfgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe451ac6dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "_ = plt.hist(np.array(freqs), normed=True, bins=100, range=(50,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [dictionary.doc2bow(sent) for sent in news_stream]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ldamodel = models.ldamodel.LdaModel(corpus, \n",
    "                                    id2word=dictionary,\n",
    "                                    num_topics=70,\n",
    "                                    passes=1,\n",
    "                                    # alpha='auto',\n",
    "                                   chunksize=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " -1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 1,\n",
       " -1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 1]"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ptrint"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
