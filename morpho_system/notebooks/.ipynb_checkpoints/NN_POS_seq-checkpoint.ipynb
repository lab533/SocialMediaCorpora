{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDEFINED = \"_\"\n",
    "MAX_WORD_LENGTH = 20\n",
    "\n",
    "\n",
    "def read_gikrya(path):\n",
    "    \"\"\"\n",
    "    Reading format:\n",
    "    row_index<TAB>form<TAB>lemma<TAB>POS<TAB>tag\n",
    "    \"\"\"\n",
    "    morpho_map = {\"POS\":{UNDEFINED: 0, \n",
    "                         0: UNDEFINED}}\n",
    "    \n",
    "    \n",
    "    sentences = []\n",
    "    vocab = {}    \n",
    "    with open(path, 'r') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            splits = line.strip().split('\\t')            \n",
    "            if len(splits) == 5:\n",
    "                form, lemma, POS, tags = splits[1:]\n",
    "                if POS not in  morpho_map[\"POS\"]:\n",
    "                    morpho_map[\"POS\"][POS] = len(morpho_map[\"POS\"]) // 2 \n",
    "                    morpho_map[\"POS\"][morpho_map[\"POS\"][POS]] =  POS\n",
    "                tags_list = [(\"POS\", POS)]\n",
    "                if tags != \"_\":\n",
    "                    for tag_val in tags.split(\"|\"):\n",
    "                        tag, val = tag_val.split(\"=\")\n",
    "                        tags_list.append((tag, val))\n",
    "                        if tag not in morpho_map:\n",
    "                            morpho_map[tag] = {UNDEFINED: 0,\n",
    "                                               0: UNDEFINED}\n",
    "                        if val not in morpho_map[tag]:\n",
    "                            morpho_map[tag][val] = len(morpho_map[tag]) // 2 \n",
    "                            morpho_map[tag][morpho_map[tag][val]] = val\n",
    "#                 else:\n",
    "#                     tags_list.append(tags)\n",
    "                if form not in vocab:\n",
    "                    vocab[form] = form\n",
    "                sentence.append((vocab[form], lemma, tags_list) )\n",
    "                \n",
    "                    \n",
    "            elif len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    pprint.pprint(morpho_map)\n",
    "    return sentences, morpho_map       \n",
    " \n",
    "\n",
    "def split_word(word, stemmer):\n",
    "    flex = word[len(stemmer.stem(word)):]\n",
    "    if len(flex):\n",
    "        return word[:-len(flex)], flex\n",
    "    return word, \"empty\"\n",
    "\n",
    "\n",
    "def sentences_to_features(sentences, token2id, neighbors=3, undef_token=\"undefined_token\"):\n",
    "    arrays = [sentence_to_features(sent, token2id,  neighbors=neighbors,\n",
    "                                  undef_token=undef_token) for sent in sentences]\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "\n",
    "def sentence_to_features(sentence, token2id, \n",
    "                         neighbors=3, undef_token=\"undefined_token\"):\n",
    "    \"\"\"\n",
    "    Делает из предложения \n",
    "    матрицу id слов, где  строка соответствует словам предложения:\n",
    "    в каждой строке состоит из neighbors id слов из левого контекста,\n",
    "    потом id слова, затем neighbors id слов правого контекста\n",
    "    0 - зарезерврован для паддинга, в словаре не должно быть слов с id 0\n",
    "    \"\"\"\n",
    "    X = np.ones(shape=(len(sentence), neighbors * 2 + 1), dtype=np.int) * len(token2id)\n",
    "    id_seq = np.zeros(shape=(len(sentence) + 2*neighbors,), dtype=np.int)\n",
    "    for idx, token in enumerate(sentence):\n",
    "        num = token2id.get(token, token2id[undef_token])\n",
    "        # assert num != 0\n",
    "        id_seq[idx+neighbors] = num\n",
    "    for idx in range(len(sentence)):\n",
    "        X[idx, :] = id_seq[idx:idx + X.shape[1]]\n",
    "    return X   \n",
    "\n",
    "        \n",
    "def build_vocab(sentences, min_freq=0, max_size=10000, undefined_id=0):\n",
    "    \"\"\" \n",
    "    Строит словарь из слов встертившихся более min_freq раз,\n",
    "    но размеров  не более max_size, в случае бОльшего количества токенов\n",
    "    отбрасываются менее частотные токены, undefined_id - id первого токена в словаре,\n",
    "    который будет называться \"undefined_token\"\n",
    "    \"\"\"\n",
    "    offset = undefined_id\n",
    "    token2id = {\"undefined_token\": offset}\n",
    "    id2token = {offset: \"undefined_token\"}    \n",
    "    \n",
    "    counter = defaultdict(int)    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            counter[token] += 1\n",
    "    sorted_tokens = [t_f[0]  for t_f in \n",
    "                     sorted([t_f for t_f in counter.items() if t_f[1] >= min_freq],\n",
    "                           key=lambda tf: -tf[1])]                     \n",
    "    \n",
    "    for token in sorted_tokens[:max_size - len(token2id)]:\n",
    "        offset += 1\n",
    "        token2id[token] = offset\n",
    "        id2token[offset] = token\n",
    "    return token2id, id2token\n",
    "\n",
    "def build_morpho_vocab(morpho_map):\n",
    "    morpho_сats = sorted([key for key in morpho_map.keys()])\n",
    "    # чисто для удобства POS сделаем первым\n",
    "    morpho_сats.insert(0, morpho_сats.pop(morpho_сats.index(\"POS\"))) \n",
    "    abs_idx = 0\n",
    "    tag2id = {}\n",
    "    id2tag = {}\n",
    "    for cat in morpho_сats:\n",
    "        vals = [pair[0] for pair in sorted(list(morpho_map[cat].items()), \n",
    "                                           key=lambda p: p[1])]\n",
    "        for val in vals:\n",
    "            tag2id[(cat, val)] = abs_idx\n",
    "            id2tag[abs_idx] = (cat, val)\n",
    "            abs_idx += 1\n",
    "    return tag2id, id2tag  \n",
    "\n",
    "\n",
    "def tagsets_to_one_hot(tagsets, morpho_map, cat_order):    \n",
    "    # при частых запусках не оптимально так:\n",
    "    # cats = set([cat for cat, val in tag2id.keys()])\n",
    "    y = [np.zeros(shape=(len(tagsets), len(morpho_map[cat]) // 2), dtype=np.int) \n",
    "         for cat in cat_order]\n",
    "    \n",
    "    for one_hot in y:\n",
    "        one_hot[:, 0] = 1       \n",
    "        \n",
    "    for idx, tagset in enumerate(tagsets):                    \n",
    "        for cat, tag in tagset:\n",
    "            # не очень эффективно индекс искать постоянно\n",
    "            \n",
    "            cat_id = cat_order.index(cat)    \n",
    "            if cat_id >= 0:\n",
    "                y[cat_id][idx, 0] = 0\n",
    "                y[cat_id][idx, morpho_map[cat].get(tag, 0)] = 1            \n",
    "    return y\n",
    "        \n",
    "    \n",
    "def preproc_dataset(full_tag_sentences, stemmer):    \n",
    "    sentences = []\n",
    "    flexes = []\n",
    "    token_tags = []\n",
    "    tokens = []\n",
    "    \n",
    "    for sent in full_tag_sentences:\n",
    "        temp_sent = []\n",
    "        temp_flexes = []\n",
    "        for token_info in sent:\n",
    "            token = token_info[0].lower()\n",
    "            tokens.append(token)\n",
    "            splits = split_word(token, stemmer)\n",
    "            temp_sent.append(splits[0])\n",
    "            temp_flexes.append(splits[1])\n",
    "            token_tags.append(token_info[2])  # надо бы переделать под стиль sentences или?          \n",
    "        sentences.append(temp_sent)\n",
    "        flexes.append(temp_flexes)    \n",
    "    return sentences, flexes, token_tags, tokens\n",
    "    \n",
    "\n",
    "       \n",
    "def make_dataset(path, stemmer,\n",
    "                      morpho_map, cat_order, undef_token,\n",
    "                      token2id, flex2id, char2id,\n",
    "                      neighbors=3):\n",
    "    \"\"\"Start here. \"\"\"\n",
    "    full_tag_sentences, _ = read_gikrya(path)    \n",
    "    sentences, flexes, token_tags, tokens = preproc_dataset(full_tag_sentences, stemmer)\n",
    "    X_stem = sentences_to_features(sentences, token2id,\n",
    "                                   neighbors=neighbors, undef_token=undef_token)\n",
    "    X_flex = sentences_to_features(flexes, flex2id, \n",
    "                                   neighbors=neighbors,\n",
    "                                   undef_token=undef_token)\n",
    "    X = chars_to_features(tokens, char2id)\n",
    "    y = tagsets_to_one_hot(token_tags, morpho_map, cat_order)\n",
    "    return X_stem, X_flex, X, y, sentences, flexes, token_tags, full_tag_sentences \n",
    "\n",
    "\n",
    "def chars_to_features(tokens, char2id):\n",
    "    X = np.zeros(shape=(len(tokens), MAX_WORD_LENGTH), dtype=np.int)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        for chid in range(min(MAX_WORD_LENGTH, len(token))):\n",
    "            X[idx, -chid-1] = char2id.get(token[-chid-1], len(char2id))\n",
    "    return X\n",
    "\n",
    "\n",
    "def add_tags_to_sentences(full_tag_sentences, y, morpho_map, cat_order):\n",
    "    new_full_tag_sents = []\n",
    "    idx = 0\n",
    "    for full_tag_sent in full_tag_sentences:\n",
    "        new_full_tag = []   \n",
    "        for token_info in full_tag_sent:\n",
    "            tags = []\n",
    "            for cat, oh_val in zip(cat_order, y):\n",
    "                \n",
    "                ntag = oh_val.shape[1]\n",
    "                tags.append((cat,\n",
    "                            [morpho_map[cat][i] for i in range(ntag) if oh_val[idx, i]==1][0]))\n",
    "            new_full_tag.append((token_info[0],\n",
    "                                '_',\n",
    "                                tags))\n",
    "            idx += 1\n",
    "        new_full_tag_sents.append(new_full_tag)\n",
    "    return new_full_tag_sents\n",
    "\n",
    "\n",
    "def probs_to_one_hot(probs):\n",
    "    one_hot = np.zeros_like(probs, dtype=np.int)\n",
    "    for row in range(one_hot.shape[0]):\n",
    "        one_hot[row, np.argmax(probs[row, :])] =1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def many_probs_to_one_hot(probs):\n",
    "    return [probs_to_one_hot(prob) for prob in probs]\n",
    "\n",
    "\n",
    "def write_gikrya(path, full_tags):\n",
    "    with open(path, 'w') as f:\n",
    "        idx = 0\n",
    "        for sentence in full_tags:\n",
    "            for i, token_info in enumerate(sentence):\n",
    "                f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i+1,\n",
    "                                                token_info[0],\n",
    "                                                token_info[1],\n",
    "                                                tagset2str(token_info[2])))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "                    \n",
    "def tagset2str(tagset):\n",
    "    POS = \"\"\n",
    "    tags_list = []\n",
    "    for tag, val in tagset:\n",
    "        if  tag == \"POS\":\n",
    "            POS = val\n",
    "        else:\n",
    "            if val != UNDEFINED:\n",
    "                tags_list.append(\"{}={}\".format(tag, val))\n",
    "    tags = \"_\"\n",
    "    if len(tags_list) > 0:\n",
    "        tags = \"|\".join(tags_list)\n",
    "    return \"{}\\t{}\".format(POS, tags)\n",
    "        \n",
    "    \n",
    "def read_embeddings(vocab_path, emb_path):    \n",
    "    token2id = {}\n",
    "    tokens = open(vocab_path, \"r\").read().strip().split(\"\\n\")    \n",
    "    for i, ch in enumerate(tokens):\n",
    "        token2id[ch] = i\n",
    "    vecs = np.load(emb_path)\n",
    "    rnd_vec = np.random.uniform(size=vecs.shape[1]) \n",
    "    return token2id, np.vstack((vecs, rnd_vec / np.linalg.norm(rnd_vec))),  tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b2dc65ffb91f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mX_stem_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_flex_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mone_hots\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmany_probs_to_one_hot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madd_tags_to_sentences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_tag_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mone_hots\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmorpho_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_order\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwrite_gikrya\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../morphoRuEval-2017/Baseline/predict/gikrya_test_nnet2.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "predicted = model.predict([X_stem_test, X_flex_test, X_test])\n",
    "one_hots = many_probs_to_one_hot(predicted)\n",
    "test = add_tags_to_sentences(full_tag_test, one_hots, morpho_map, cat_order)\n",
    "write_gikrya(\"../morphoRuEval-2017/Baseline/predict/gikrya_test_nnet2.txt\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = model_from_json(open('../models/model_3/model_json.arch', 'r').read())\n",
    "model.load_weights('../models/model_4/weights.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-73d359931200>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,    token_tags, full_tag_test = make_dataset(path, stemmer, \n\u001b[0m\u001b[1;32m      8\u001b[0m                                                 \u001b[0mmorpho_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_order\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mundef_token\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                                 token2id, flex2id, char2id, neighbors=neighbors)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "prefix = \"../morphoRuEval-2017/test_collection/\" \n",
    "# tests = [\"VK.txt\"]#, \"JZ.txt\", \"Lenta.txt\"]\n",
    "tests = [\"JZ.txt\", \"Lenta.txt\"]\n",
    "\n",
    "for p in tests:\n",
    "    path = os.path.join(prefix, p)\n",
    "    X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,\\\n",
    "    token_tags, full_tag_test = make_dataset(path, stemmer, \n",
    "                                                morpho_map, cat_order,undef_token, \n",
    "                                                token2id, flex2id, char2id, neighbors=neighbors)\n",
    "    predicted = model.predict([X_stem_test, X_flex_test, X_test], batch_size=512)\n",
    "    one_hots = many_probs_to_one_hot(predicted)\n",
    "    test = add_tags_to_sentences(full_tag_test, one_hots, morpho_map, cat_order)\n",
    "    write_gikrya(\"../morphoRuEval-2017/test_collection/tagged/{}\".format(p), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"../models/\"\n",
    "morpho_map, cat_order =  pickle.load(open(\"../models/morpho.pickle\", 'rb'))\n",
    "token2id, token_vecs, undef_token = read_embeddings(os.path.join(prefix, \"stem2id\"),\n",
    "                                      os.path.join(prefix, \"stem_embeddings.npy\"))\n",
    "\n",
    "flex2id, flex_vecs, _ = read_embeddings(os.path.join(prefix, \"flex2id\"),\n",
    "                                     os.path.join(prefix, \"flex_embeddings.npy\"))\n",
    "\n",
    "# lemm2id, lemm_vecs, _ = read_embeddings(os.path.join(prefix, \"lemm2id\"),\n",
    "#                                      os.path.join(prefix, \"lemm_embeddings.npy\"))\n",
    "\n",
    "char2id = {}\n",
    "chars = open(\"char2id\", \"r\").read().strip().split(\"\\n\")\n",
    "for i, ch in enumerate(chars):\n",
    "    char2id[ch] = i\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'undefined_token'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id\n",
    "id2char = {chid:ch for ch, chid in char2id.items()}\n",
    "id2char[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbors = 3\n",
    "gikrya_path = \"../morphoRuEval-2017/Baseline/source/gikrya_train.txt\"\n",
    "# gikrya_path = \"../JointMorphoClosed.txt\"\n",
    "X_stem_train, X_flex_train, X_train, y_train, sentences, flexes,\\\n",
    "token_tags, full_tag_sentences=make_dataset(gikrya_path, stemmer, \n",
    "                                            morpho_map, cat_order, undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gikrya_test = \"../morphoRuEval-2017/Baseline/source/gikrya_test.txt\"\n",
    "X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,\\\n",
    "token_tags, full_tag_test = make_dataset(gikrya_test, stemmer, \n",
    "                                            morpho_map, cat_order,undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_stem, X_flex, X = np.vstack((X_stem_train, X_stem_test)),\\\n",
    "                                   np.vstack((X_flex_train, X_flex_test)),\\\n",
    "                                   np.vstack((X_train, X_test))\n",
    "\n",
    "y = [np.vstack((train, test)) for train, test in zip(y_train, y_test)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A PLAN:\n",
    "\n",
    "- запилить POS tag only, вход - слово, выход - POS\n",
    "- запилить POS tag only, вход - последовательность слов, выход - последовательность POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gpuarray/__init__.py\", line 164, in <module>\n",
      "    use(config.device)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gpuarray/__init__.py\", line 151, in use\n",
      "    init_dev(device)\n",
      "  File \"/usr/local/lib/python3.5/dist-packages/theano/gpuarray/__init__.py\", line 60, in init_dev\n",
      "    sched=config.gpuarray.sched)\n",
      "  File \"pygpu/gpuarray.pyx\", line 614, in pygpu.gpuarray.init (pygpu/gpuarray.c:9419)\n",
      "  File \"pygpu/gpuarray.pyx\", line 566, in pygpu.gpuarray.pygpu_init (pygpu/gpuarray.c:9110)\n",
      "  File \"pygpu/gpuarray.pyx\", line 1021, in pygpu.gpuarray.GpuContext.__cinit__ (pygpu/gpuarray.c:13472)\n",
      "pygpu.gpuarray.GpuArrayException: Unknown device error: -1\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, Bidirectional, Merge\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,\n",
    "                           output_dim=char2vec_dim)       \n",
    "\n",
    "encoded_char = Bidirectional(LSTM(char_hidden,\n",
    "                                   dropout_U=0.2, \n",
    "                                   dropout_W=0.2))(char_embedding(char_in))    \n",
    "\n",
    "\n",
    "stem_in = Input(shape=(X_stem_train.shape[1],))\n",
    "stem_embedding = Embedding(input_dim=token_vecs.shape[0],\n",
    "                           output_dim=token_vecs.shape[1],\n",
    "                           weights=[token_vecs])       \n",
    "\n",
    "encoded_stem = LSTM(stem_hidden,\n",
    "                    dropout_U=0.2, \n",
    "                    dropout_W=0.2)(stem_embedding(stem_in))    \n",
    "\n",
    "\n",
    "# flex_in = Input(shape=(X_flex_train.shape[1],))\n",
    "# flex_embedding = Embedding(input_dim=flex_vecs.shape[0],\n",
    "#                            output_dim=flex_vecs.shape[1],\n",
    "#                            weights=[flex_vecs])       \n",
    "\n",
    "# encoded_flex = LSTM(char_hidden,\n",
    "#                     dropout_U=0.2, \n",
    "#                     dropout_W=0.2)(flex_embedding(flex_in))    \n",
    "\n",
    "\n",
    "merged = keras.layers.merge([encoded_char, encoded_stem], mode='concat')\n",
    "# merged = keras.layers.merge([encoded_char, encoded_flex], mode='concat')\n",
    "pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1], \n",
    "            activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model([char_in, stem_in], pos_predict)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  model.fit  происходит здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_fraction = 0.07\n",
    "shuffled_indicies = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffled_indicies)\n",
    "split_index = int(X.shape[0] * (1 - test_fraction))\n",
    "train = shuffled_indicies[:split_index]\n",
    "test = shuffled_indicies[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010117 samples, validate on 76031 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "model.fit([X[train], X_flex[train]], y[cat_order.index('POS')][train],          \n",
    "          validation_data=([X[test], X_flex[test]], y[cat_order.index('POS')][test]),          \n",
    "          batch_size=batch_size,           \n",
    "          nb_epoch=epochs, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010117 samples, validate on 76031 samples\n",
      "Epoch 1/10\n",
      "547s - loss: 0.0855 - acc: 0.9664 - val_loss: 0.0887 - val_acc: 0.9660\n",
      "Epoch 2/10\n",
      "548s - loss: 0.0856 - acc: 0.9663 - val_loss: 0.0886 - val_acc: 0.9664\n",
      "Epoch 3/10\n",
      "549s - loss: 0.0858 - acc: 0.9664 - val_loss: 0.0877 - val_acc: 0.9663\n",
      "Epoch 4/10\n",
      "549s - loss: 0.0859 - acc: 0.9664 - val_loss: 0.0908 - val_acc: 0.9670\n",
      "Epoch 5/10\n",
      "549s - loss: 0.0862 - acc: 0.9663 - val_loss: 0.0901 - val_acc: 0.9668\n",
      "Epoch 6/10\n",
      "548s - loss: 0.0863 - acc: 0.9661 - val_loss: 0.0879 - val_acc: 0.9676\n",
      "Epoch 7/10\n",
      "548s - loss: 0.0871 - acc: 0.9661 - val_loss: 0.0895 - val_acc: 0.9667\n",
      "Epoch 8/10\n",
      "548s - loss: 0.0874 - acc: 0.9660 - val_loss: 0.0909 - val_acc: 0.9636\n",
      "Epoch 9/10\n",
      "548s - loss: 0.0873 - acc: 0.9662 - val_loss: 0.0952 - val_acc: 0.9635\n",
      "Epoch 10/10\n",
      "548s - loss: 0.0878 - acc: 0.9659 - val_loss: 0.0928 - val_acc: 0.9629\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc6369e32b0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 256\n",
    "epochs = 10\n",
    "model.fit(X[train], y[cat_order.index('POS')][train],          \n",
    "          validation_data=(X[test], y[cat_order.index('POS')][test]),          \n",
    "          batch_size=batch_size, nb_epoch=epochs, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A PLAN:\n",
    "- ~~запилить POS tag only, вход - слово, выход - POS~~\n",
    "- попробовать помимо самого слова скармливать весь контекст посимвольно\n",
    "- потестить гиперпараметры\n",
    "- придумать способ сэмплирования последовательности, чтобы сэмпл был из одного предложения и тэгги сразу проставлялись для всей последовательности\n",
    "- запилить POS tag only, вход - последовательность слов, выход - последовательность POS\n",
    "- потестить гиперпараметры\n",
    "- сравнить подходы\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results:\n",
    "- простой вариант:\n",
    "\n",
    "> token_hidden = 128\n",
    "\n",
    "> flex_hidden = 64\n",
    "\n",
    "> char2vec_dim = 42\n",
    "\n",
    "> char_hidden = 512\n",
    "\n",
    "> char_in = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "> char_embedding = Embedding(input_dim=len(char2id) + 1,                           \n",
    ">                            output_dim=char2vec_dim)       \n",
    "encoded_char = Bidirectional(LSTM(char_hidden,\n",
    ">                            dropout_U=0.2, \n",
    ">                            dropout_W=0.2))(char_embedding(char_in))    \n",
    "\n",
    "> pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1], \n",
    ">                     activation='softmax',)(encoded_char)\n",
    "\n",
    "результаты: лучший за 20 эпох 0.976\n",
    "\n",
    "- со стеммами:\n",
    "\n",
    "> token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1, output_dim=char2vec_dim)       \n",
    "encoded_char = Bidirectional(LSTM(char_hidden,                                   dropout_U=0.2, dropout_W=0.2))(char_embedding(char_in))    \n",
    "stem_in = Input(shape=(X_stem_train.shape[1],))\n",
    "stem_embedding = Embedding(input_dim=token_vecs.shape[0], output_dim=token_vecs.shape[1],  weights=[token_vecs])       \n",
    "encoded_stem = LSTM(stem_hidden, dropout_U=0.2, dropout_W=0.2)(stem_embedding(stem_in))    \n",
    "\n",
    "результаты: лучший за 10 эпох 0.9874\n",
    "\n",
    "- с флексиями:\n",
    "\n",
    "> token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,                           output_dim=char2vec_dim)       \n",
    "encoded_char = Bidirectional(LSTM(char_hidden,                                   dropout_U=0.2,  dropout_W=0.2))(char_embedding(char_in))    \n",
    "flex_in = Input(shape=(X_flex_train.shape[1],))\n",
    "flex_embedding = Embedding(input_dim=flex_vecs.shape[0],                           output_dim=flex_vecs.shape[1], weights=[flex_vecs])     \n",
    "encoded_flex = LSTM(char_hidden, dropout_U=0.2, dropout_W=0.2)(flex_embedding(flex_in))    \n",
    "merged = keras.layers.merge([encoded_char, encoded_flex], mode='concat')\n",
    "pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1],             activation='softmax')(merged)\n",
    "\n",
    "результаты: лучший за 10 эпох 0.9773"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
