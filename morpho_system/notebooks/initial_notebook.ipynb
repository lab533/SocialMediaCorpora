{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# np.random.randint(0, high=10)\n",
    "# !export LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/cuda/lib64:/usr/local/cuda/extras/CUPTI/lib64\"\n",
    "# !export CUDA_HOME=/usr/local/cuda\n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import random\n",
    "import pprint\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "\n",
    "class Evaluation(object):\n",
    "\n",
    "    def __init__(self, golden_set, system_set):\n",
    "        \"\"\"Format of golden and system_set: array with little arrays of on length for every word\n",
    "        Like [['plen', 'inan', 'ipf', 'nom', 'comp', 'm', 'indic', 'sg', 'persn', 'S-PRO', '3p',\n",
    "        'praet', 'tran', 'ger', 'act'], \n",
    "        ['plen', 'inan', 'ipf', 'nom', 'comp', 'm', 'indic', 'sg', 'persn', 'S', '1p', \n",
    "        'praet', 'intr', 'ger', 'act'],...]\n",
    "        \"\"\"\n",
    "        self.golden_set = golden_set\n",
    "        self.predicted_set = system_set\n",
    "        self.tags_dictionary = defaultdict(int)\n",
    "        self.golden_tags_dict = defaultdict(int)\n",
    "        self.all_number = len(golden_set)\n",
    "        self.accuracy_real_tag = 0\n",
    "\n",
    "        for one_array in golden_set:\n",
    "            for one_tag in one_array:\n",
    "                self.golden_tags_dict[one_tag] += 1\n",
    "\n",
    "    def count_accuracy(self):\n",
    "        for gold, system in zip(self.golden_set, self.predicted_set):\n",
    "            one_word_set = []\n",
    "            for i in range(len(gold)):\n",
    "                if gold[i] == system[i]:\n",
    "                    self.tags_dictionary[gold[i]] += 1\n",
    "                    one_word_set.append(1)\n",
    "                else:\n",
    "                    if gold[i] == 'UNDEFINED':\n",
    "                        one_word_set.append(1)\n",
    "            if len(one_word_set) == len(gold):\n",
    "                self.accuracy_real_tag += 1\n",
    "\n",
    "        print(\"ALL tags: \", self.accuracy_real_tag, self.all_number, self.accuracy_real_tag/self.all_number)\n",
    "\n",
    "        for key, value in sorted(self.tags_dictionary.items()):\n",
    "            real_tag_number = self.golden_tags_dict.get(key)\n",
    "            print(\"One tag accuracy: \", key, value, real_tag_number, value/real_tag_number)\n",
    "\n",
    "#=====================\n",
    "\n",
    "GRADIENT_CLIP_NORM = 15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def make_projection(tensor, output_dim=None, input_dim=None):\n",
    "    if input_dim is None:\n",
    "        input_dim = tensor.get_shape()[-1].value\n",
    "    if output_dim is None:\n",
    "        output_dim = input_dim\n",
    "    s = tf.shape(tensor)\n",
    "    rank = len(tensor.get_shape())\n",
    "    assert rank in (2,3)\n",
    "    if rank == 2:\n",
    "        inshape = tf.pack([s[0], input_dim])\n",
    "        outshape = tf.pack([s[0], output_dim])\n",
    "    elif rank == 3:\n",
    "        inshape = tf.pack([s[0] * s[1], input_dim])\n",
    "        outshape = tf.pack([s[0], s[1], output_dim])\n",
    "    flat_tensor = tf.reshape(tensor, inshape)\n",
    "    proj = tf.Variable(tf.truncated_normal([input_dim, output_dim], dtype=tf.float32, stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([output_dim], dtype=tf.float32))\n",
    "    projected_tensor = tf.matmul(flat_tensor, proj) + bias\n",
    "    result = tf.reshape(projected_tensor, outshape)\n",
    "    if rank == 2:\n",
    "        result.set_shape([None, output_dim])\n",
    "    elif rank == 3:\n",
    "        result.set_shape([None, None, output_dim])\n",
    "    return result\n",
    "\n",
    "def bidirectional_dynamic_rnn(cell_forward, initial_state_forward, cell_backward, initial_state_backward, inputs, sequence_lengths):\n",
    "    with tf.variable_scope(\"forward\"):\n",
    "        forward, fw_state = tf.nn.dynamic_rnn(cell=cell_forward, inputs=inputs, sequence_length=sequence_lengths, initial_state=initial_state_forward,\n",
    "                                dtype=tf.float32, time_major=True, swap_memory=True)\n",
    "    with tf.variable_scope(\"backward\"):\n",
    "        sequence_lengths64 = tf.cast(sequence_lengths, dtype=tf.int64) # weird requirement of tf.reverse_sequence\n",
    "        inputs_reversed = tf.reverse_sequence(input=inputs, seq_lengths=sequence_lengths64, seq_dim=0, batch_dim=1)\n",
    "        backward_reversed, bw_state = tf.nn.dynamic_rnn(cell=cell_backward, inputs=inputs_reversed, sequence_length=sequence_lengths, initial_state=initial_state_backward,\n",
    "                                dtype=tf.float32, time_major=True, swap_memory=True)\n",
    "        backward = tf.reverse_sequence(input=backward_reversed, seq_lengths=sequence_lengths64, seq_dim=0, batch_dim=1)\n",
    "    output = tf.concat(2, [forward, backward])\n",
    "    return output, fw_state, bw_state\n",
    "\n",
    "def get_optimizer(optimizer, loss):\n",
    "    gradients, v = zip(*optimizer.compute_gradients(loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, GRADIENT_CLIP_NORM)\n",
    "    return optimizer.apply_gradients(zip(gradients, v))\n",
    "\n",
    "def compute_masked_multihead_loss(bottleneck, target, mask):\n",
    "    masks = tf.unpack(tf.transpose(mask, perm=(2, 0, 1)))\n",
    "    targets = tf.unpack(tf.transpose(target, perm=(2, 0, 1)))\n",
    "    assert len(cats) == len(masks)\n",
    "    result = [] # will be list of triples (mean cross-entropy, number of correct top-1 choices, number of valid examples)\n",
    "    logits = {}\n",
    "    for i,(cat, num_targets) in enumerate(catlens):\n",
    "        logits_i = make_projection(bottleneck, output_dim=num_targets)\n",
    "        mask_i = masks[i]\n",
    "        target_i = targets[i]\n",
    "        xent_i = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_i, labels=target_i)\n",
    "        correct_i = tf.nn.in_top_k(predictions=tf.reshape(logits_i, [-1, num_targets]), targets=tf.reshape(target_i, [-1]), k=1)\n",
    "        xent_i = tf.reduce_mean(tf.boolean_mask(xent_i, mask_i))\n",
    "        correct_i = tf.reduce_sum(tf.cast(tf.boolean_mask(correct_i, tf.reshape(mask_i, [-1])), tf.int32))\n",
    "        n_i = tf.reduce_sum(tf.cast(mask_i, tf.int32))\n",
    "        xent_i = tf.select(tf.equal(n_i, 0), 0.0, xent_i)\n",
    "        result.append((xent_i, correct_i, n_i))\n",
    "        logits[cat] = tf.exp(logits_i) / tf.reduce_sum(tf.exp(logits_i), keep_dims=True, reduction_indices=2)\n",
    "    return result, logits\n",
    "\n",
    "#=============================================================\n",
    "\n",
    "def word2vec(word):\n",
    "    \"\"\"Transform word into a fixed-sized int32 vector (MAX_WORD_LENGTH) with values\n",
    "    up to len(charmap)\"\"\"\n",
    "    result = np.zeros([MAX_WORD_LENGTH], dtype=np.int32)\n",
    "    word = [charmap[\"BEGIN\"]] + list(word) + [charmap[\"END\"]]\n",
    "    if len(word) > MAX_WORD_LENGTH:\n",
    "        prefix, suffix = word[:MAX_PS_LENGTH], word[-MAX_PS_LENGTH:]\n",
    "        word = prefix + suffix\n",
    "    for i, c in enumerate(word):\n",
    "        result[i] = charmap[c] if c in charmap else charmap[\"UNKNOWN\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "def sentence2example(sentence):\n",
    "    \"\"\"Take a sentence: [(word, feats)] and stack these vectors into two matrices, correspondingly\"\"\"\n",
    "    ws, fs = list(zip(*sentence))\n",
    "    return np.stack(ws), np.stack(fs)\n",
    "\n",
    "\n",
    "def make_dataset(fn):\n",
    "    dataset = []\n",
    "    print(fn)\n",
    "    with open(fn, 'r') as f:\n",
    "        sentence = []\n",
    "        for l in f:\n",
    "            l = l.strip().split()\n",
    "            assert len(l) == len(cats) + 1\n",
    "            if l[0] == \"SENTENCE\":\n",
    "                assert cats == tuple(l[1:])  # check that the ordering is the same\n",
    "                if len(sentence):\n",
    "                    dataset.append(sentence2example(sentence))\n",
    "                sentence = []\n",
    "            else:\n",
    "                word = word2vec(l[0])\n",
    "                feats = np.asarray(list(map(int, l[1:])), dtype=np.int32)\n",
    "                sentence.append((word, feats))\n",
    "    if len(sentence):\n",
    "        dataset.append(sentence2example(sentence))  # the last example has no closing tag\n",
    "    # print(dataset)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def read_gikrya(path):\n",
    "    \"\"\"\n",
    "    Reading format:\n",
    "    row_index<TAB>form<TAB>lemma<TAB>POS<TAB>tag\"\"\"\n",
    "    tags_map = {}\n",
    "    POS_map = {}\n",
    "    sentences = []\n",
    "    with open(path, 'r') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            splits = line.strip().split('\\t')            \n",
    "            if len(splits) == 5:\n",
    "                form, lemma, POS, tags = splits[1:]\n",
    "                if POS not in POS_map:\n",
    "                    POS_map[POS] = len(POS_map) \n",
    "                tags_list = []\n",
    "                if tags != \"_\":\n",
    "                    for tag_val in tags.split(\"|\"):\n",
    "                        tag, val = tag_val.split(\"=\")\n",
    "                        tags_list.append((tag, val))\n",
    "                        if tag not in tags_map:\n",
    "                            tags_map[tag] = {}\n",
    "                        if val not in tags_map[tag]:\n",
    "                            tags_map[tag][val] = len(tags_map[tag])                            \n",
    "                else:\n",
    "                    tags_list.append(tags)\n",
    "                sentence.append((form, lemma, POS, tags_list) )\n",
    "            elif len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    return sentences, POS_map, tags_map       \n",
    "                    \n",
    "    \n",
    "def build_vocab(gikrya_sents, max_words=None, min_freq=None):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = \"../morphoRuEval-2017/Baseline/source/gikrya_train.txt\"\n",
    "sentences, POS_map, tags_map = read_gikrya(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18, 20), (18, 15))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sentences[-1]\n",
    "devset[0][0].shape, devset[0][1].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "MAX_WORDS_IN_BATCH = 200\n",
    "\n",
    "\n",
    "# Actually this is *not* a real upper bound -- if a sentence is longer than MAX_WORDS_IN_BATCH,\n",
    "# a 1-example minibatch will be formed solely by this sentence to avoid data loss.\n",
    "# Be aware of this if you face OOM errors.\n",
    "\n",
    "def stack_and_mask(examples_list):\n",
    "    \"\"\"CONVENTION to save memory: mask (i.e. valid inputs) is equivalent to non-zero targets.\"\"\"\n",
    "\n",
    "    def pad(x, to_length):\n",
    "        if x.shape[0] == to_length: return x\n",
    "        return np.pad(x, pad_width=((0, to_length - x.shape[0]), (0, 0)), mode='constant', constant_values=0)\n",
    "\n",
    "    feats, targets = list(zip(*examples_list))\n",
    "    sequence_lengths = list(map(lambda x: x.shape[0], feats))\n",
    "    maxlen = max(sequence_lengths)\n",
    "    feats, targets = list(map(lambda mats: np.stack([pad(x, maxlen) for x in mats]), [feats, targets]))\n",
    "    feats, targets = list(map(lambda x: x.swapaxes(0, 1), [feats, targets]))  # Time x Batch x Features\n",
    "    return feats, targets, np.asarray(sequence_lengths, dtype=np.int32)\n",
    "\n",
    "\n",
    "def batcher(dataset):\n",
    "    batch, t = [], 0\n",
    "    for d in dataset:\n",
    "        batch.append(d)\n",
    "        t = max(t, d[0].shape[0])\n",
    "        # because of padding, all elems in the batch have the same length\n",
    "        # (which is equal to the max elem length)\n",
    "        if t * len(batch) < MAX_WORDS_IN_BATCH:\n",
    "            batch.append(d)\n",
    "        else:\n",
    "            yield stack_and_mask(batch)  # actually, we can do it later and yield just the batch list\n",
    "            batch, t = [], 0\n",
    "    if len(batch):\n",
    "        yield stack_and_mask(batch)\n",
    "\n",
    "        raise StopIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#==========================================\n",
    "\n",
    "prefix = \"../../Anglicizm/taggers/\"\n",
    "\n",
    "random.seed(777)\n",
    "\n",
    "MAX_PS_LENGTH = 10 # maximal prefix or suffix length\n",
    "MAX_WORD_LENGTH = 2 * MAX_PS_LENGTH\n",
    "# words shorter than MAX_WORD_LENGTH will be truncated in the middle\n",
    "# so that both prefix and suffix are MAX_PS_LENGTH\n",
    "\n",
    "charmap = {\"EMPTY\":0, \"BEGIN\":1, \"END\":2}\n",
    "with open(os.path.join(prefix, \"chars.txt\"), 'r') as charfile:\n",
    "    num = len(charmap)\n",
    "    for line in charfile.readlines():\n",
    "        char = line.split()[0]\n",
    "        for ch in char.split('|'):\n",
    "            charmap[ch] = num\n",
    "        num += 1\n",
    "charmap[\"UNKNOWN\"] = num\n",
    "\n",
    "# pprint.pprint(sorted(charmap.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('AdForm', 3),\n",
      " ('Animacy', 3),\n",
      " ('Aspect', 3),\n",
      " ('Case', 13),\n",
      " ('Degree', 4),\n",
      " ('Gender', 5),\n",
      " ('Mood', 4),\n",
      " ('Number', 3),\n",
      " ('Other', 6),\n",
      " ('POS', 19),\n",
      " ('Person', 4),\n",
      " ('Tense', 4),\n",
      " ('Transition', 3),\n",
      " ('VerbForm', 4),\n",
      " ('Voice', 4)]\n"
     ]
    }
   ],
   "source": [
    "catlens = Counter()\n",
    "catmap = defaultdict(dict)\n",
    "\n",
    "with open(os.path.join(prefix,\"output_map.txt\"), 'r') as mf:\n",
    "    for l in mf:\n",
    "        cat, catname, catint = l.strip().split()\n",
    "        catlens.update([cat])\n",
    "        catmap[cat][int(catint)] = catname\n",
    "catlens = sorted(catlens.most_common())\n",
    "\n",
    "#[('Gender', 4),('Mood', 3), ('Number', 3), ..]\n",
    "pprint.pprint(catlens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Animacy': {'Anim': 1, 'Inan': 0},\n",
       "  'Case': {'Acc': 2, 'Dat': 1, 'Gen': 5, 'Ins': 4, 'Loc': 3, 'Nom': 0},\n",
       "  'Degree': {'Cmp': 1, 'Pos': 0},\n",
       "  'Form': {'Digit': 0},\n",
       "  'Gender': {'Fem': 0, 'Masc': 1, 'Neut': 2},\n",
       "  'Mood': {'Imp': 1, 'Ind': 0},\n",
       "  'Number': {'Plur': 1, 'Sing': 0},\n",
       "  'Person': {'1': 1, '2': 2, '3': 0},\n",
       "  'Tense': {'Notpast': 1, 'Past': 0, 'Pres': 2},\n",
       "  'Variant': {'Short': 0},\n",
       "  'VerbForm': {'Conv': 2, 'Fin': 0, 'Inf': 1},\n",
       "  'Voice': {'Act': 0, 'Mid': 1}},\n",
       " {'ADJ': 8,\n",
       "  'ADP': 4,\n",
       "  'ADV': 10,\n",
       "  'CONJ': 7,\n",
       "  'DET': 0,\n",
       "  'H': 11,\n",
       "  'INTJ': 12,\n",
       "  'NOUN': 1,\n",
       "  'NUM': 9,\n",
       "  'PART': 6,\n",
       "  'PRON': 3,\n",
       "  'PUNCT': 5,\n",
       "  'VERB': 2})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags_map, POS_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(dict,\n",
       "            {'AdForm': {0: 'UNDEFINED', 1: 'brev', 2: 'plen'},\n",
       "             'Animacy': {0: 'UNDEFINED', 1: 'anim', 2: 'inan'},\n",
       "             'Aspect': {0: 'UNDEFINED', 1: 'pf', 2: 'ipf'},\n",
       "             'Case': {0: 'UNDEFINED',\n",
       "              1: 'nom',\n",
       "              2: 'voc',\n",
       "              3: 'gen',\n",
       "              4: 'gen2',\n",
       "              5: 'dat',\n",
       "              6: 'acc',\n",
       "              7: 'dat2',\n",
       "              8: 'ins',\n",
       "              9: 'loc',\n",
       "              10: 'loc2',\n",
       "              11: 'acc2',\n",
       "              12: 'adnum'},\n",
       "             'Degree': {0: 'UNDEFINED', 1: 'comp', 2: 'supr', 3: 'comp2'},\n",
       "             'Gender': {0: 'UNDEFINED', 1: 'm', 2: 'f', 3: 'n', 4: 'm-f'},\n",
       "             'Mood': {0: 'UNDEFINED', 1: 'indic', 2: 'imper', 3: 'imper2'},\n",
       "             'Number': {0: 'UNDEFINED', 1: 'sg', 2: 'pl'},\n",
       "             'Other': {0: 'UNDEFINED',\n",
       "              1: 'persn',\n",
       "              2: 'patrn',\n",
       "              3: 'famn',\n",
       "              4: 'zoon',\n",
       "              5: '0'},\n",
       "             'POS': {0: 'UNDEFINED',\n",
       "              1: 'A',\n",
       "              2: 'A-NUM',\n",
       "              3: 'A-PRO',\n",
       "              4: 'ADV',\n",
       "              5: 'ADV-PRO',\n",
       "              6: 'CONJ',\n",
       "              7: 'INIT',\n",
       "              8: 'INTJ',\n",
       "              9: 'NONLEX',\n",
       "              10: 'NUM',\n",
       "              11: 'PARENTH',\n",
       "              12: 'PART',\n",
       "              13: 'PR',\n",
       "              14: 'PRAEDIC',\n",
       "              15: 'PRAEDIC-PRO',\n",
       "              16: 'S',\n",
       "              17: 'S-PRO',\n",
       "              18: 'V'},\n",
       "             'Person': {0: 'UNDEFINED', 1: '1p', 2: '2p', 3: '3p'},\n",
       "             'Tense': {0: 'UNDEFINED', 1: 'praet', 2: 'praes', 3: 'fut'},\n",
       "             'Transition': {0: 'UNDEFINED', 1: 'tran', 2: 'intr'},\n",
       "             'VerbForm': {0: 'UNDEFINED', 1: 'inf', 2: 'partcp', 3: 'ger'},\n",
       "             'Voice': {0: 'UNDEFINED', 1: 'act', 2: 'pass', 3: 'med'}})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "catmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../Anglicizm/taggers/train_half.txt\n",
      "../../Anglicizm/taggers/dev_half.txt\n",
      "../../Anglicizm/taggers/test_half.txt\n",
      "127807 85204 42449\n",
      "[8334, 4140]\n"
     ]
    }
   ],
   "source": [
    "cats = list(zip(*catlens))[0]\n",
    "\n",
    "trainfn, devfn, testfn = list(map(lambda x: os.path.join(prefix, x) + \"_half.txt\", [\"train\", \"dev\", \"test\"]))\n",
    "trainset, devset, testset = list(map(make_dataset, [trainfn, devfn, testfn])) # already matrix\n",
    "print(len(trainset), len(devset), len(testset))\n",
    "\n",
    "def get_batches(dataset, randomize):\n",
    "    sorted_dataset = sorted(dataset, key=lambda x: random.random() if randomize else x[0].shape[0])\n",
    "    return [b for b in batcher(sorted_dataset)]\n",
    "\n",
    "dev, test = list(map(lambda x: get_batches(x, randomize=False), [devset, testset]))\n",
    "print(list(map(len, [dev, test])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 201, 15)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#===========NN======================================\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "model_filename = \"/home/user1/projects/Anglicizm/taggers/mystem_half.model.chkpt\"\n",
    "\n",
    "alphabet_size = len(charmap)\n",
    "num_outputs = len(cats)\n",
    "\n",
    "char_embedding_size = alphabet_size\n",
    "conv_filter_widths = [3, 4, 5]  # shingle sizes\n",
    "num_conv_maps = 128\n",
    "rnn_cell_size = 128\n",
    "rnn_projection_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    # inputs\n",
    "    input_words = tf.placeholder(tf.int32, shape=(None, None, MAX_WORD_LENGTH))  # time x batch x chars\n",
    "    input_sequence_lengths = tf.placeholder(tf.int32, shape=(None,))  # batch\n",
    "    output_targets = tf.placeholder(tf.int32, shape=(None, None, num_outputs))  # time x batch x output_id\n",
    "    output_targets_mask = tf.placeholder(tf.bool, shape=(None, None, num_outputs))  # time x batch x output_id\n",
    "\n",
    "    # shapes\n",
    "    max_sequence_length = tf.shape(input_words)[0]\n",
    "    batch_size = tf.shape(input_words)[1]\n",
    "\n",
    "    # computation\n",
    "\n",
    "    # first we embed chars into some dense space\n",
    "    char_embeddings = tf.Variable(tf.random_uniform([alphabet_size, char_embedding_size],\n",
    "                                                    minval=-np.sqrt(3), maxval=np.sqrt(3)),dtype=tf.float32)\n",
    "\n",
    "    input_words_embedded = tf.nn.embedding_lookup(params=char_embeddings,ids=input_words)  # time x batch x chars x feats\n",
    "\n",
    "    # now we convolve over them to reduce dimensionality\n",
    "    rnn_inputs = []\n",
    "    for conv_filter_width in conv_filter_widths:\n",
    "        filter_tensor = tf.Variable(\n",
    "            tf.truncated_normal([1, conv_filter_width, char_embedding_size, num_conv_maps], stddev=0.1),dtype=tf.float32)  # filter_height x filter_width x in_channels x out_channels\n",
    "        input_words_shingled = tf.nn.conv2d(input=input_words_embedded, filter=filter_tensor,\n",
    "                                            strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        input_words_shingled_pooled = tf.nn.max_pool(value=input_words_shingled,\n",
    "                                                     ksize=[1, 1, input_words_shingled.get_shape()[2], 1],\n",
    "                                                     strides=[1, 1, 1, 1], padding=\"VALID\")\n",
    "        input_words_shingled_pooled = tf.squeeze(input_words_shingled_pooled, squeeze_dims=[2])\n",
    "        rnn_inputs.append(input_words_shingled_pooled)\n",
    "    rnn_inputs = tf.concat(2, rnn_inputs)\n",
    "\n",
    "    # ready to build the rnn\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(num_units=rnn_cell_size, num_proj=rnn_projection_size, state_is_tuple=True,initializer=None)\n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(num_units=rnn_cell_size, num_proj=rnn_projection_size, state_is_tuple=True,initializer=None)\n",
    "    rnn_output, _, _ = bidirectional_dynamic_rnn(cell_forward=cell_fw, initial_state_forward=None,\n",
    "                                                 cell_backward=cell_bw, initial_state_backward=None,\n",
    "                                                 inputs=rnn_inputs, sequence_lengths=input_sequence_lengths)\n",
    "\n",
    "    multihead_loss, predictions = compute_masked_multihead_loss(bottleneck=rnn_output, target=output_targets,\n",
    "                                                                mask=output_targets_mask)\n",
    "    xents, correct_counts, ns = zip(*multihead_loss)\n",
    "    loss = tf.reduce_sum(xents)\n",
    "    accuracies = {cat: (correct_counts[i], ns[i]) for i, cat in enumerate(cats)}\n",
    "    optimizer = get_optimizer(tf.train.AdamOptimizer(learning_rate=1e-4), loss)\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "num_epochs = 15\n",
    "best_val = None\n",
    "logfile = open(\"mystem_half_logger.log\", \"w\")\n",
    "\n",
    "from itertools import starmap\n",
    "\n",
    "def log(obj):\n",
    "    print (obj)\n",
    "    print(logfile, obj)\n",
    "\n",
    "with tf.Session(graph=graph, config=tf.ConfigProto(log_device_placement=True)) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    # saver.restore(session, model_filename)\n",
    "    print('Initialized')\n",
    "\n",
    "    def do_epoch(batches, do_backprop, mode):\n",
    "        acc_loss, counter = np.float128(0), np.uint64(0)\n",
    "        guesses = [[np.uint64(0), np.uint64(0)] for cat in cats]\n",
    "        num_steps = len(batches)\n",
    "        for step, batch in enumerate(batches):\n",
    "            feats, tgts, seq_lens = batch\n",
    "            feed_dict = {\n",
    "                input_words: feats,\n",
    "                input_sequence_lengths: seq_lens,\n",
    "                output_targets: tgts,\n",
    "                output_targets_mask: (tgts != 0)\n",
    "            }\n",
    "            acc_fetches = [accuracies[cat] for cat in cats] #correct_counts\n",
    "\n",
    "            if do_backprop:\n",
    "                fetch = session.run([optimizer, loss] + acc_fetches, feed_dict=feed_dict)\n",
    "                step_loss = fetch[1]\n",
    "                accs = fetch[2:]\n",
    "            else:\n",
    "                fetch = session.run([loss] + acc_fetches, feed_dict=feed_dict)\n",
    "                step_loss = fetch[0]\n",
    "                accs = fetch[1:]\n",
    "            acc_loss += step_loss * seq_lens.sum()\n",
    "            counter += seq_lens.sum()\n",
    "            for i, (correct, n) in enumerate(accs):\n",
    "                guesses[i][0] += correct\n",
    "                guesses[i][1] += n\n",
    "            if step%100 == 0:    \n",
    "                print (\"\\rAverage loss at step %d / %d: %f\" % (step, num_steps, acc_loss / counter))\n",
    "        acc_loss /= counter\n",
    "        guesses = sorted(zip(cats, list(starmap(lambda x,y: (round(x / y, 3), int(y)) if y != 0 else \"UNDEFINED\", guesses))))\n",
    "        res = \"%s: epoch avg loss: % f\" % (mode, acc_loss)\n",
    "        log(res)\n",
    "        log(guesses)\n",
    "        return acc_loss\n",
    "\n",
    "    for epoch in range(num_epochs + 1):\n",
    "        print(\"Starting epoch %d\" % epoch)\n",
    "        val = do_epoch(dev, do_backprop=False, mode=\"VALID\") # здесь оцениваем модель, обученную на train, на dev сете\n",
    "        if best_val is None:\n",
    "            # saverpath = saver.save(session, model_filename)\n",
    "            # print(saverpath)\n",
    "            # saver.restore(session, model_filename)\n",
    "            best_val = val\n",
    "        elif best_val < val:\n",
    "            saver.restore(session, model_filename)\n",
    "        elif best_val > val:\n",
    "            best_val = val\n",
    "            saver.save(session, model_filename)\n",
    "        if epoch != num_epochs:\n",
    "            train = get_batches(trainset, randomize=True)\n",
    "            tr = do_epoch(train, do_backprop=True, mode=\"TRAIN\") # обучаем модель\n",
    "        logfile.flush()\n",
    "\n",
    "    saver.restore(session, model_filename)\n",
    "    log(\"FINAL RUN ON DEDICATED TESTSET\")\n",
    "    do_epoch(test, do_backprop=False, mode=\"TEST\")\n",
    "\n",
    "\n",
    "    def do_test(text):\n",
    "\n",
    "        POS_results = []\n",
    "        system_results = []\n",
    "        words = text.split()\n",
    "        inputs = np.stack([word2vec(word) for word in words])\n",
    "        inputs = np.expand_dims(inputs, 1)\n",
    "        lens = inputs.shape[0] * np.ones([1])\n",
    "        with tf.Session(graph=graph) as session:\n",
    "            saver.restore(session, model_filename)\n",
    "            feed_dict = {\n",
    "                input_words: inputs,\n",
    "                input_sequence_lengths: lens,\n",
    "            }\n",
    "            res = session.run(list(predictions.values()), feed_dict=feed_dict)\n",
    "            res = list(map(lambda x: (np.argmax(x, 2), np.max(x, 2)), res))\n",
    "            res = sorted(zip(predictions.keys(), res))\n",
    "            for i, w in enumerate(words):\n",
    "                # print('Word: ', w)\n",
    "                one_word_res = []\n",
    "                for line in res:\n",
    "                    cat = line[0]\n",
    "                    smth = line[1]\n",
    "                    indexs, scores = smth[0], smth[1]\n",
    "                    # print(cat, catmap[cat][indexs[i][0]], scores[i][0])\n",
    "                    one_word_res.append(catmap[cat][indexs[i][0]])\n",
    "                    if cat == 'POS':\n",
    "                        POS_results.append(w+\"_\"+catmap[cat][indexs[i][0]])\n",
    "                system_results.append(one_word_res)\n",
    "        #print(system_results)\n",
    "        #print(' '.join(POS_results))\n",
    "        return system_results\n",
    "\n",
    "\n",
    "    main_cats = 'AdForm Animacy Aspect Case Degree Gender Mood Number Other POS Person Tense Transition VerbForm Voice'\n",
    "    with open('test_processed_corpus.txt', 'r', encoding='utf-8') as my_testset:\n",
    "        golden_tags = []  # array of words where every word is another array and has gold categories\n",
    "        golden_words = []\n",
    "        for line in my_testset.readlines():\n",
    "            if 'SENTENCE' not in line:\n",
    "                word = line.split()[0]\n",
    "                categ = line.split()[1:]\n",
    "                word_tags = []\n",
    "                for real_cat, value in zip(main_cats.split(), categ):\n",
    "                    tag = catmap[real_cat][int(value)]\n",
    "                    word_tags.append(tag)\n",
    "                golden_tags.append(word_tags)\n",
    "                golden_words.append(word)\n",
    "        #print(golden_words)\n",
    "        #print(golden_tags)\n",
    "\n",
    "    system_results = do_test(' '.join(golden_words))\n",
    "    # do_test(u\"всё на свете должно происходить медленно и неправильно чтобы не сумел загордиться человек чтобы человек был грустен и растерян .\")\n",
    "    print(len(golden_tags), len(system_results))\n",
    "\n",
    "    eval_obj = Evaluation(golden_tags, system_results)\n",
    "    eval_obj.count_accuracy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
