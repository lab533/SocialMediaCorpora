{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "UNDEFINED_TOKEN = \"undefined_token\"\n",
    "MAX_WORD_LENGTH = 20\n",
    "UNDEFINED = \"_\"\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "class LowerSentencesWithoutStops(object):\n",
    "    def __init__(self, fnames, token2token, stops):\n",
    "        self.fnames = fnames        \n",
    "        self.token2token = token2token         \n",
    "        \n",
    "    def __iter__(self):\n",
    "        for fname in self.fnames:\n",
    "            for line in open(fname, 'r', encoding=\"utf8\"):            \n",
    "                yield [self.token2token.get(token, UNDEFINED_TOKEN)\n",
    "                       for token in line.lower().split()]\n",
    "                \n",
    "                \n",
    "\n",
    "    \n",
    "def split_word(word, stemmer):\n",
    "    flex = word[len(stemmer.stem(word)):]\n",
    "    if len(flex):\n",
    "        return word[:-len(flex)], flex\n",
    "    return word, \"empty\"\n",
    "\n",
    "\n",
    "def build_vocab(sentences, min_freq=0, max_size=10000, undefined_id=0):\n",
    "    \"\"\" \n",
    "    Строит словарь из слов встертившихся более min_freq раз,\n",
    "    но размеров  не более max_size, в случае бОльшего количества токенов\n",
    "    отбрасываются менее частотные токены, undefined_id - id первого токена в словаре,\n",
    "    который будет называться \"undefined_token\"\n",
    "    \"\"\"\n",
    "    offset = undefined_id\n",
    "    token2id = {UNDEFINED_TOKEN: offset}\n",
    "    id2token = {offset: UNDEFINED_TOKEN}    \n",
    "    \n",
    "    counter = defaultdict(int)    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            counter[token] += 1\n",
    "    sorted_tokens = [t_f[0]  for t_f in \n",
    "                     sorted([t_f for t_f in counter.items() if t_f[1] >= min_freq],\n",
    "                           key=lambda tf: -tf[1])]                     \n",
    "    \n",
    "    for token in sorted_tokens[:max_size - len(token2id)]:\n",
    "        offset += 1\n",
    "        token2id[token] = offset\n",
    "        id2token[offset] = token\n",
    "    return token2id, id2token \n",
    "\n",
    "\n",
    "def build_ch_vocab(text, min_freq=0, max_size=100, undefined_id=0):\n",
    "    \"\"\" \n",
    "    Строит словарь из слов встертившихся более min_freq раз,\n",
    "    но размеров  не более max_size, в случае бОльшего количества токенов\n",
    "    отбрасываются менее частотные токены, undefined_id - id первого токена в словаре,\n",
    "    который будет называться \"undefined_token\"\n",
    "    \"\"\"\n",
    "    offset = undefined_id\n",
    "    token2id = {UNDEFINED_TOKEN: offset}\n",
    "    id2token = {offset: UNDEFINED_TOKEN}    \n",
    "    \n",
    "    counter = defaultdict(int)    \n",
    "    for token in text:\n",
    "        counter[token] += 1\n",
    "        \n",
    "    sorted_tokens = [t_f[0]  for t_f in \n",
    "                     sorted([t_f for t_f in counter.items() if t_f[1] >= min_freq],\n",
    "                           key=lambda tf: -tf[1])]                     \n",
    "    \n",
    "    for token in sorted_tokens[:max_size - len(token2id)]:\n",
    "        offset += 1\n",
    "        token2id[token] = offset\n",
    "        id2token[offset] = token\n",
    "    return token2id, id2token  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def read_gikrya(path):\n",
    "    \"\"\"\n",
    "    Reading format:\n",
    "    row_index<TAB>form<TAB>lemma<TAB>POS<TAB>tag\n",
    "    \"\"\"\n",
    "    \n",
    "    morpho_map = {\"POS\":{UNDEFINED: 0, \n",
    "                         0: UNDEFINED}}\n",
    "    sentences = []\n",
    "    vocab = {}    \n",
    "    with open(path, 'r') as f:\n",
    "        \n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            splits = line.strip().split('\\t')      \n",
    "            if len(splits) == 4:\n",
    "                splits.insert(0, 1)\n",
    "            if len(splits) == 5:\n",
    "                form, lemma, POS, tags = splits[1:]\n",
    "                if POS not in  morpho_map[\"POS\"]:\n",
    "                    morpho_map[\"POS\"][POS] = len(morpho_map[\"POS\"]) // 2 \n",
    "                    morpho_map[\"POS\"][morpho_map[\"POS\"][POS]] =  POS\n",
    "                tags_list = [(\"POS\", POS)]\n",
    "                if tags != \"_\":\n",
    "                    for tag_val in tags.split(\"|\"):\n",
    "                        tag, val = tag_val.split(\"=\")\n",
    "                        tags_list.append((tag, val))\n",
    "                        if tag not in morpho_map:\n",
    "                            morpho_map[tag] = {UNDEFINED: 0,\n",
    "                                               0: UNDEFINED}\n",
    "                        if val not in morpho_map[tag]:\n",
    "                            morpho_map[tag][val] = len(morpho_map[tag]) // 2 \n",
    "                            morpho_map[tag][morpho_map[tag][val]] = val\n",
    "                if form not in vocab:\n",
    "                    vocab[form] = form\n",
    "                sentence.append((vocab[form], lemma, tags_list) )\n",
    "            elif len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    return sentences, morpho_map \n",
    "\n",
    "\n",
    "def read_corpus(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.strip().lower().split())\n",
    "    return sentences\n",
    "        \n",
    "    \n",
    "def write_vecs(path, vecs_path, id2token, w2v_model):\n",
    "    # косяк с тем чтон undefined token не 0\n",
    "    vecs = np.zeros(shape=(len(token2id), w2v_model.vector_size))\n",
    "    with open(path, 'w') as f:\n",
    "        for tid in range(len(id2token)):\n",
    "            vecs[tid, :] = w2v_model[id2token[tid]]\n",
    "            f.write(id2token[tid])\n",
    "            f.write(\"\\n\")\n",
    "    np.save(vecs_path, vecs)\n",
    "    \n",
    "\n",
    "def preproc_dataset(full_tag_sentences, stemmer):    \n",
    "    sentences = []\n",
    "    flexes = []\n",
    "    token_tags = []\n",
    "    \n",
    "    for sent in full_tag_sentences:\n",
    "        temp_sent = []\n",
    "        temp_flexes = []\n",
    "        for token_info in sent:\n",
    "            token = token_info[0].lower()          \n",
    "            splits = split_word(token, stemmer)\n",
    "            temp_sent.append(splits[0])\n",
    "            temp_flexes.append(splits[1])\n",
    "            token_tags.append(token_info[2])  # надо бы переделать под стиль sentences или?          \n",
    "        sentences.append(temp_sent)\n",
    "        flexes.append(temp_flexes)    \n",
    "    return sentences, flexes, token_tags\n",
    "\n",
    "\n",
    "def get_tokens(sentences):\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "    \n",
    "    \n",
    "def preproc_files(fnames):\n",
    "    sentences_full = []\n",
    "    for fname in fnames:\n",
    "        s_full, _ = read_gikrya(fname)\n",
    "        sentences_full = sentences_full + s_full\n",
    "    return sentences_full\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-192-55e063fa5fbd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tagged = \"../morphoRuEval-2017/Baseline/source/gikrya_train.txt\"\n",
    "path_to_write_morpho = \"../models/morpho.pickle\"\n",
    "sentences_full, morpho_map = read_gikrya(path_to_tagged)\n",
    "cat_order = sorted([key for key in morpho_map.keys()])\n",
    "pickle.dump((morpho_map, cat_order), open(path_to_write_morpho, 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = [\"../JointMorphoClosed.txt\", \n",
    "          \"../morphoRuEval-2017/test_collection/VK.txt\",\n",
    "         \"../morphoRuEval-2017/test_collection/JZ.txt\",\n",
    "          \"../morphoRuEval-2017/test_collection/Lenta.txt\"]\n",
    "\n",
    "sentences_full = preproc_files(fnames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# morpho_map\n",
    "# !head  \"../JointMorphoClosed.txt\"\n",
    "# len(stem_modeiil.vocab)\n",
    "sentences, flexes, token_tags = preproc_dataset(sentences_full, stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "char2id, id2char = build_ch_vocab(open(\"../JointMorphoClosed.txt\", \"r\").read().lower(),\n",
    "                                  max_size=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"char2id\", \"w\") as f:\n",
    "    for i in range(len(char2id)):\n",
    "        f.write(\"{}\\n\".format(id2char[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\t',\n",
       " '\\n',\n",
       " '!',\n",
       " '\"',\n",
       " '#',\n",
       " '%',\n",
       " '&',\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " '0',\n",
       " '1',\n",
       " '2',\n",
       " '3',\n",
       " '4',\n",
       " '5',\n",
       " '6',\n",
       " '7',\n",
       " '8',\n",
       " '9',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '[',\n",
       " ']',\n",
       " '_',\n",
       " 'a',\n",
       " 'b',\n",
       " 'c',\n",
       " 'd',\n",
       " 'e',\n",
       " 'f',\n",
       " 'g',\n",
       " 'h',\n",
       " 'i',\n",
       " 'j',\n",
       " 'k',\n",
       " 'l',\n",
       " 'm',\n",
       " 'n',\n",
       " 'o',\n",
       " 'p',\n",
       " 'r',\n",
       " 's',\n",
       " 't',\n",
       " 'u',\n",
       " 'undefined_token',\n",
       " 'v',\n",
       " 'w',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z',\n",
       " '|',\n",
       " '«',\n",
       " '»',\n",
       " 'а',\n",
       " 'б',\n",
       " 'в',\n",
       " 'г',\n",
       " 'д',\n",
       " 'е',\n",
       " 'ж',\n",
       " 'з',\n",
       " 'и',\n",
       " 'й',\n",
       " 'к',\n",
       " 'л',\n",
       " 'м',\n",
       " 'н',\n",
       " 'о',\n",
       " 'п',\n",
       " 'р',\n",
       " 'с',\n",
       " 'т',\n",
       " 'у',\n",
       " 'ф',\n",
       " 'х',\n",
       " 'ц',\n",
       " 'ч',\n",
       " 'ш',\n",
       " 'щ',\n",
       " 'ъ',\n",
       " 'ы',\n",
       " 'ь',\n",
       " 'э',\n",
       " 'ю',\n",
       " 'я',\n",
       " 'ё',\n",
       " '–',\n",
       " '—',\n",
       " '•',\n",
       " '…',\n",
       " '№']"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([ch for ch in char2id.keys()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101\r\n"
     ]
    }
   ],
   "source": [
    "!cat char2id | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "363470"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_path = '../for_embedding/allTexts_stemmas.txt'\n",
    "flex_path = '../for_embedding/allTexts_flexias.txt'\n",
    "\n",
    "stemmas = read_corpus(stem_path)\n",
    "flexias = read_corpus(flex_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmas = get_tokens(stemmas)\n",
    "flexias = get_tokens(flexias)\n",
    "# len(set(stemmas)), len(set(flexias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(612715, 768)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(stemmas)), len(set(flexias))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514, 26)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id, id2token = build_vocab(stemmas, \n",
    "                                 min_freq=1,\n",
    "                                 max_size=80000)\n",
    "\n",
    "flex2id, id2flex = build_vocab(flexias, \n",
    "                               min_freq=2, \n",
    "                               max_size=500)\n",
    "\n",
    "len(token2id), len(flex2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stem2stem = {}\n",
    "for stem in token2id.keys():\n",
    "    stem2stem[stem] = stem\n",
    "    \n",
    "flex2flex = {}\n",
    "for flex in flex2id.keys():\n",
    "    flex2flex[flex] = flex\n",
    "\n",
    "new_sents = [[stem2stem.get(token, UNDEFINED_TOKEN) for token in sent] for sent in sentences]\n",
    "new_flexes = [[flex2flex.get(token, UNDEFINED_TOKEN) for token in sent] for sent in flexes]\n",
    "len(stem2stem), len(flex2flex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del stem_sentences, flex_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stem_sentences = list(LowerSentencesWithoutStops([stem_path], stem2stem, set([])))\n",
    "# stem_model = Word2Vec(stem_sentences, size=128, sg=1, workers=5, iter=10, min_count=1)\n",
    "stem_model = Word2Vec(new_sents, size=200, sg=1, workers=5, iter=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['undefined_token',\n",
       " 35,\n",
       " 127,\n",
       " 7,\n",
       " 1199,\n",
       " 3,\n",
       " 31520,\n",
       " 38671,\n",
       " 1,\n",
       " 16489,\n",
       " 556,\n",
       " 4598,\n",
       " 50,\n",
       " 9006,\n",
       " 1,\n",
       " 9,\n",
       " 100,\n",
       " 89,\n",
       " 23]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_sents[0]\n",
    "# stem_model.vector_size\n",
    "# print([stiiem for stem, i in token2id.items() if stem not in stem_model])\n",
    "# x = [flex for flex, i in flex2id.items() if flex not in flex_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flex_sentences = list(LowerSentencesWithoutStops([flex_path], flex2flex, set([])))\n",
    "# flex_model = Word2Vec(flex_sentences, size=128, sg=1, workers=5, iter=10, min_count=1)\n",
    "flex_model = Word2Vec(new_flexes, size=128, sg=1, workers=5, iter=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"../models\"\n",
    "\n",
    "\n",
    "write_vecs(os.path.join(prefix,\"stem2id\"),\n",
    "           os.path.join(prefix, \"stem_embeddings\"),\n",
    "           id2token, stem_model)\n",
    "\n",
    "\n",
    "write_vecs(os.path.join(prefix, \"flex2id\"), \n",
    "           os.path.join(prefix, \"flex_embeddings\"),\n",
    "           id2flex, flex_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(id2token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"asd asd asd\\n\".split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token2id[UNDEFINED_TOKEN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "undefined_token\r\n",
      ",\r\n",
      ".\r\n",
      "и\r\n",
      "в\r\n",
      "не\r\n",
      "-\r\n",
      "на\r\n",
      "эт\r\n",
      "что\r\n"
     ]
    }
   ],
   "source": [
    "!head \"../models/stem2id\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'empty'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2flex[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'e', 'q', 'r', 'w'}"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
