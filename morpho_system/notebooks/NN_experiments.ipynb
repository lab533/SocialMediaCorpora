{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "UNDEFINED = \"_\"\n",
    "MAX_WORD_LENGTH = 20\n",
    "\n",
    "\n",
    "def read_gikrya(path):\n",
    "    \"\"\"\n",
    "    Reading format:\n",
    "    row_index<TAB>form<TAB>lemma<TAB>POS<TAB>tag\n",
    "    \"\"\"\n",
    "    morpho_map = {\"POS\":{UNDEFINED: 0, \n",
    "                         0: UNDEFINED}}\n",
    "    \n",
    "    \n",
    "    sentences = []\n",
    "    vocab = {}    \n",
    "    with open(path, 'r') as f:\n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            splits = line.strip().split('\\t')            \n",
    "            if len(splits) == 5:\n",
    "                form, lemma, POS, tags = splits[1:]\n",
    "                if POS not in  morpho_map[\"POS\"]:\n",
    "                    morpho_map[\"POS\"][POS] = len(morpho_map[\"POS\"]) // 2 \n",
    "                    morpho_map[\"POS\"][morpho_map[\"POS\"][POS]] =  POS\n",
    "                tags_list = [(\"POS\", POS)]\n",
    "                if tags != \"_\":\n",
    "                    for tag_val in tags.split(\"|\"):\n",
    "                        tag, val = tag_val.split(\"=\")\n",
    "                        tags_list.append((tag, val))\n",
    "                        if tag not in morpho_map:\n",
    "                            morpho_map[tag] = {UNDEFINED: 0,\n",
    "                                               0: UNDEFINED}\n",
    "                        if val not in morpho_map[tag]:\n",
    "                            morpho_map[tag][val] = len(morpho_map[tag]) // 2 \n",
    "                            morpho_map[tag][morpho_map[tag][val]] = val\n",
    "#                 else:\n",
    "#                     tags_list.append(tags)\n",
    "                if form not in vocab:\n",
    "                    vocab[form] = form\n",
    "                sentence.append((vocab[form], lemma, tags_list) )\n",
    "                \n",
    "                    \n",
    "            elif len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    return sentences, morpho_map       \n",
    " \n",
    "    \n",
    "def split_word(word, stemmer):\n",
    "    flex = word[len(stemmer.stem(word)):]\n",
    "    if len(flex):\n",
    "        return word[:-len(flex)], flex\n",
    "    return word, \"empty\"\n",
    "\n",
    "\n",
    "def sentences_to_features(sentences, token2id, neighbors=3, undef_token=\"undefined_token\"):\n",
    "    arrays = [sentence_to_features(sent, token2id,  neighbors=neighbors,\n",
    "                                  undef_token=undef_token) for sent in sentences]\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "\n",
    "def sentence_to_features(sentence, token2id, \n",
    "                         neighbors=3, undef_token=\"undefined_token\"):\n",
    "    \"\"\"\n",
    "    Делает из предложения \n",
    "    матрицу id слов, где  строка соответствует словам предложения:\n",
    "    в каждой строке состоит из neighbors id слов из левого контекста,\n",
    "    потом id слова, затем neighbors id слов правого контекста\n",
    "    0 - зарезерврован для паддинга, в словаре не должно быть слов с id 0\n",
    "    \"\"\"\n",
    "    X = np.ones(shape=(len(sentence), neighbors * 2 + 1), dtype=np.int) * len(token2id)\n",
    "    id_seq = np.zeros(shape=(len(sentence) + 2*neighbors,), dtype=np.int)\n",
    "    for idx, token in enumerate(sentence):\n",
    "        num = token2id.get(token, token2id[undef_token])\n",
    "        # assert num != 0\n",
    "        id_seq[idx+neighbors] = num\n",
    "    for idx in range(len(sentence)):\n",
    "        X[idx, :] = id_seq[idx:idx + X.shape[1]]\n",
    "    return X   \n",
    "        \n",
    "        \n",
    "def build_vocab(sentences, min_freq=0, max_size=10000, undefined_id=0):\n",
    "    \"\"\" \n",
    "    Строит словарь из слов встертившихся более min_freq раз,\n",
    "    но размеров  не более max_size, в случае бОльшего количества токенов\n",
    "    отбрасываются менее частотные токены, undefined_id - id первого токена в словаре,\n",
    "    который будет называться \"undefined_token\"\n",
    "    \"\"\"\n",
    "    offset = undefined_id\n",
    "    token2id = {\"undefined_token\": offset}\n",
    "    id2token = {offset: \"undefined_token\"}    \n",
    "    \n",
    "    counter = defaultdict(int)    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            counter[token] += 1\n",
    "    sorted_tokens = [t_f[0]  for t_f in \n",
    "                     sorted([t_f for t_f in counter.items() if t_f[1] >= min_freq],\n",
    "                           key=lambda tf: -tf[1])]                     \n",
    "    \n",
    "    for token in sorted_tokens[:max_size - len(token2id)]:\n",
    "        offset += 1\n",
    "        token2id[token] = offset\n",
    "        id2token[offset] = token\n",
    "    return token2id, id2token      \n",
    "    \n",
    "    \n",
    "    \n",
    "def simple_word2vec(word):\n",
    "    pass\n",
    "\n",
    "\n",
    "def build_morpho_vocab(morpho_map):\n",
    "    morpho_сats = sorted([key for key in morpho_map.keys()])\n",
    "    # чисто для удобства POS сделаем первым\n",
    "    morpho_сats.insert(0, morpho_сats.pop(morpho_сats.index(\"POS\"))) \n",
    "    abs_idx = 0\n",
    "    tag2id = {}\n",
    "    id2tag = {}\n",
    "    for cat in morpho_сats:\n",
    "        vals = [pair[0] for pair in sorted(list(morpho_map[cat].items()), \n",
    "                                           key=lambda p: p[1])]\n",
    "        for val in vals:\n",
    "            tag2id[(cat, val)] = abs_idx\n",
    "            id2tag[abs_idx] = (cat, val)\n",
    "            abs_idx += 1\n",
    "    return tag2id, id2tag  \n",
    "\n",
    "\n",
    "def tagsets_to_one_hot(tagsets, morpho_map, cat_order):    \n",
    "    # при частых запусках не оптимально так:\n",
    "    # cats = set([cat for cat, val in tag2id.keys()])\n",
    "    y = [np.zeros(shape=(len(tagsets), len(morpho_map[cat]) // 2), dtype=np.int) \n",
    "         for cat in cat_order]\n",
    "    \n",
    "    for one_hot in y:\n",
    "        one_hot[:, 0] = 1       \n",
    "        \n",
    "    for idx, tagset in enumerate(tagsets):                    \n",
    "        for cat, tag in tagset:\n",
    "            # не очень эффективно индекс искать постоянно\n",
    "            \n",
    "            cat_id = cat_order.index(cat)    \n",
    "            if cat_id >= 0:\n",
    "                y[cat_id][idx, 0] = 0\n",
    "                y[cat_id][idx, morpho_map[cat].get(tag, 0)] = 1            \n",
    "    return y\n",
    "        \n",
    "    \n",
    "def preproc_dataset(full_tag_sentences, stemmer):    \n",
    "    sentences = []\n",
    "    flexes = []\n",
    "    token_tags = []\n",
    "    tokens = []\n",
    "    \n",
    "    for sent in full_tag_sentences:\n",
    "        temp_sent = []\n",
    "        temp_flexes = []\n",
    "        for token_info in sent:\n",
    "            token = token_info[0].lower()\n",
    "            tokens.append(token)\n",
    "            splits = split_word(token, stemmer)\n",
    "            temp_sent.append(splits[0])\n",
    "            temp_flexes.append(splits[1])\n",
    "            token_tags.append(token_info[2])  # надо бы переделать под стиль sentences или?          \n",
    "        sentences.append(temp_sent)\n",
    "        flexes.append(temp_flexes)    \n",
    "    return sentences, flexes, token_tags, tokens\n",
    "    \n",
    "\n",
    "       \n",
    "def make_dataset(path, stemmer,\n",
    "                      morpho_map, cat_order, undef_token,\n",
    "                      token2id, flex2id, char2id,\n",
    "                      neighbors=3):\n",
    "    full_tag_sentences, _ = read_gikrya(path)    \n",
    "    sentences, flexes, token_tags, tokens = preproc_dataset(full_tag_sentences, stemmer)\n",
    "    X_stem = sentences_to_features(sentences, token2id,\n",
    "                                   neighbors=neighbors, undef_token=undef_token)\n",
    "    X_flex = sentences_to_features(flexes, flex2id, \n",
    "                                   neighbors=neighbors,\n",
    "                                   undef_token=undef_token)\n",
    "    X = chars_to_features(tokens, char2id)\n",
    "    y = tagsets_to_one_hot(token_tags, morpho_map, cat_order)\n",
    "    return X_stem, X_flex, X, y, sentences, flexes, token_tags, full_tag_sentences \n",
    "\n",
    "\n",
    "def chars_to_features(tokens, char2id):\n",
    "    X = np.ones(shape=(len(tokens), MAX_WORD_LENGTH), dtype=np.int) * len(char2id)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        for chid in range(min(MAX_WORD_LENGTH, len(token))):\n",
    "            X[idx, chid] = char2id.get(token[-chid-1], 0)\n",
    "    return X        \n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "def add_tags_to_sentences(full_tag_sentences, y, morpho_map, cat_order):\n",
    "    new_full_tag_sents = []\n",
    "    idx = 0\n",
    "    for full_tag_sent in full_tag_sentences:\n",
    "        new_full_tag = []   \n",
    "        for token_info in full_tag_sent:\n",
    "            tags = []\n",
    "            for cat, oh_val in zip(cat_order, y):\n",
    "                \n",
    "                ntag = oh_val.shape[1]\n",
    "                tags.append((cat,\n",
    "                            [morpho_map[cat][i] for i in range(ntag) if oh_val[idx, i]==1][0]))\n",
    "            new_full_tag.append((token_info[0],\n",
    "                                '_',\n",
    "                                tags))\n",
    "            idx += 1\n",
    "        new_full_tag_sents.append(new_full_tag)\n",
    "    return new_full_tag_sents\n",
    "\n",
    "\n",
    "def probs_to_one_hot(probs):\n",
    "    one_hot = np.zeros_like(probs, dtype=np.int)\n",
    "    for row in range(one_hot.shape[0]):\n",
    "        one_hot[row, np.argmax(probs[row, :])] =1\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def many_probs_to_one_hot(probs):\n",
    "    return [probs_to_one_hot(prob) for prob in probs]\n",
    "\n",
    "\n",
    "def write_gikrya(path, full_tags):\n",
    "    with open(path, 'w') as f:\n",
    "        idx = 0\n",
    "        for sentence in full_tags:\n",
    "            for i, token_info in enumerate(sentence):\n",
    "                f.write(\"{}\\t{}\\t{}\\t{}\\n\".format(i+1,\n",
    "                                                token_info[0],\n",
    "                                                token_info[1],\n",
    "                                                tagset2str(token_info[2])))\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "                \n",
    "\n",
    "                    \n",
    "def tagset2str(tagset):\n",
    "    POS = \"\"\n",
    "    tags_list = []\n",
    "    for tag, val in tagset:\n",
    "        if  tag == \"POS\":\n",
    "            POS = val\n",
    "        else:\n",
    "            if val != UNDEFINED:\n",
    "                tags_list.append(\"{}={}\".format(tag, val))\n",
    "    tags = \"_\"\n",
    "    if len(tags_list) > 0:\n",
    "        tags = \"|\".join(tags_list)\n",
    "    return \"{}\\t{}\".format(POS, tags)\n",
    "        \n",
    "    \n",
    "def read_embeddings(vocab_path, emb_path):    \n",
    "    token2id = {}\n",
    "    tokens = open(vocab_path, \"r\").read().strip().split(\"\\n\")    \n",
    "    for i, ch in enumerate(tokens):\n",
    "        token2id[ch] = i\n",
    "    vecs = np.load(emb_path)\n",
    "    rnd_vec = np.random.uniform(size=vecs.shape[1]) \n",
    "    return token2id, np.vstack((vecs, rnd_vec / np.linalg.norm(rnd_vec))),  tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'f'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"sdfa\"[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# test part\n",
    "# token2id, id2token = buil_vocab(sentences_for_w2v, min_freq=1000, max_size=1000)\n",
    "# sent = sentences_for_w2v[0]\n",
    "# print(\" \".join(sent))\n",
    "# print(\" \".join(map(lambda t: str(token2id.get(t)), sent)))\n",
    "# sentence_to_features(sent, token2id)\n",
    "# full_tag_sentences, morpho_map = read_gikrya(gikrya_path)\n",
    "# gikrya_path = \"../morphoRuEval-2017/Baseline/source/gikrya_train.txt\"\n",
    "# full_tag_sentences, morpho_map = read_gikrya(gikrya_path)\n",
    "# tag2id, id2tag = build_morpho_vocab(morpho_map)\n",
    "# cat_order = sorted([key for key in morpho_map.keys()])\n",
    "# token_tags[0]\n",
    "# yy = tagsets_to_one_hot(token_tags, morpho_map, cat_order)\n",
    "# morpho_map[\"POS\"][\"NOUN\"]\n",
    "# one_hots = many_probs_to_one_hot(predicted)\n",
    "# test_predicted_ft = add_tags_to_sentences()\n",
    "# test_predicted = add_tags_to_sentences(full_tag_test, one_hots, morpho_map, cat_order)\n",
    "# write_gikrya(\"../morphoRuEval-2017/Baseline/predict/gikrya_test_nnet.txt\", test_predicted)\n",
    "# s =[[(x, 0, 0) for x in \"\"\"Комната\n",
    "# в\n",
    "# весёлых\n",
    "# пробегах\n",
    "# огней\n",
    "# .\"\"\".split(\"\\n\")]]\n",
    "\n",
    "# x, xf, _ = preproc_dataset(s,  stemmer)\n",
    "# x = sentences_to_features(x, token2id)\n",
    "# xf = sentences_to_features(xf, flex2id)\n",
    "\n",
    "predicted = model.predict([X_stem_test, X_flex_test, X_test])\n",
    "one_hots = many_probs_to_one_hot(predicted)\n",
    "test = add_tags_to_sentences(full_tag_test, one_hots, morpho_map, cat_order)\n",
    "write_gikrya(\"../morphoRuEval-2017/Baseline/predict/gikrya_test_nnet2.txt\", test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# model = model_from_json(open('../models/model_3/model_json.arch', 'r').read())\n",
    "model.load_weights('../models/model_4/weights.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prefix = \"../morphoRuEval-2017/test_collection/\" \n",
    "# tests = [\"VK.txt\"]#, \"JZ.txt\", \"Lenta.txt\"]\n",
    "tests = [\"JZ.txt\", \"Lenta.txt\"]\n",
    "\n",
    "for p in tests:\n",
    "    path = os.path.join(prefix, p)\n",
    "    X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,\\\n",
    "    token_tags, full_tag_test = make_dataset(path, stemmer, \n",
    "                                                morpho_map, cat_order,undef_token, \n",
    "                                                token2id, flex2id, char2id, neighbors=neighbors)\n",
    "    predicted = model.predict([X_stem_test, X_flex_test, X_test], batch_size=512)\n",
    "    one_hots = many_probs_to_one_hot(predicted)\n",
    "    test = add_tags_to_sentences(full_tag_test, one_hots, morpho_map, cat_order)\n",
    "    write_gikrya(\"../morphoRuEval-2017/test_collection/tagged/{}\".format(p), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prefix = \"../models/\"\n",
    "morpho_map, cat_order =  pickle.load(open(\"../models/morpho.pickle\", 'rb'))\n",
    "token2id, token_vecs, undef_token = read_embeddings(os.path.join(prefix, \"stem2id\"),\n",
    "                                      os.path.join(prefix, \"stem_embeddings.npy\"))\n",
    "\n",
    "flex2id, flex_vecs, _ = read_embeddings(os.path.join(prefix, \"flex2id\"),\n",
    "                                     os.path.join(prefix, \"flex_embeddings.npy\"))\n",
    "\n",
    "# lemm2id, lemm_vecs, _ = read_embeddings(os.path.join(prefix, \"lemm2id\"),\n",
    "#                                      os.path.join(prefix, \"lemm_embeddings.npy\"))\n",
    "\n",
    "char2id = {}\n",
    "chars = open(\"char2id\", \"r\").read().strip().split(\"\\n\")\n",
    "for i, ch in enumerate(chars):\n",
    "    char2id[ch] = i\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "char2id[\"o\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "neighbors = 3\n",
    "gikrya_path = \"../morphoRuEval-2017/Baseline/source/gikrya_train.txt\"\n",
    "# gikrya_path = \"../JointMorphoClosed.txt\"\n",
    "X_stem_train, X_flex_train, X_train, y_train, sentences, flexes,\\\n",
    "token_tags, full_tag_sentences=make_dataset(gikrya_path, stemmer, \n",
    "                                            morpho_map, cat_order, undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "gikrya_test = \"../morphoRuEval-2017/Baseline/source/gikrya_test.txt\"\n",
    "X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,\\\n",
    "token_tags, full_tag_test = make_dataset(gikrya_test, stemmer, \n",
    "                                            morpho_map, cat_order,undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((815884, 20), (815884, 7), (815884, 20), 13)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_flex_train.shape, X_train.shape, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# X_train.shape, X_flex_train.shape, X_train.shape, len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((270264, 20), (270264, 11), (270264, 20), 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_flex_test.shape, X_test.shape, len(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 450, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(token2id), len(flex2id), len(char2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_stem, X_flex, X = np.vstack((X_stem_train, X_stem_test)),\\\n",
    "                                   np.vstack((X_flex_train, X_flex_test)),\\\n",
    "                                   np.vstack((X_train, X_test))\n",
    "\n",
    "y = [np.vstack((train, test)) for train, test in zip(y_train, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50223, 128)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_vecs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A PLAN:\n",
    "Итак пока идея такая: берем [mnist_hierarchical_rnn.py](https://github.com/fchollet/keras/blob/master/examples/mnist_hierarchical_rnn.py) \n",
    "и юзаем\n",
    "На вход подаем вектор стемма и отдельно вектор флексии, думаю для этого можно использовать\n",
    "embedding layers\n",
    "\n",
    "TO DO:\n",
    "- ~~запилить хотя бы просто CountVectorizer для формирования интов на входы\n",
    "embeddings~~ \n",
    "\n",
    "> запилил build_vocab\n",
    "\n",
    "- ~~запилить преобразования целевых тэгов в onehot или еще как~~\n",
    "- ~~поднять сеть HierarchicalRNN со стеммаим на входе  и только POS на выходе~~\n",
    "- ~~допилить туда флексии~~\n",
    "- ~~попробовать поменять LSTM на SimpleRNN~~\n",
    "\n",
    "> учиться чуть быстрее(примерно в 4 раза) результаты чуть медленнее сходятся (примерно в 2 раз)..\n",
    "\n",
    "- ~~попробовать GRU ?~~\n",
    "- ~~допилить классификаторы для остальных тэггов~~\n",
    "- ~~make ud format output for eval task results~~\n",
    "- ~~прописать коллбэки для earl\n",
    "y_stopping и сохранения лучшей модели~~\n",
    "- попробовать контекст побольше?\n",
    "- настройка геракла\n",
    "- сделать замену пунктуации на тэг PUNCT\n",
    "- сделать скрипт обучения модели, вход: [коллекция, директория для модели, [тест для валидации]]\n",
    "выход: сохраненная модель\n",
    "- сделать скрипт разметки вход: [коллекция, модель] выход: ud разметка\n",
    "- проверить UNDEFINED\n",
    "- ???\n",
    "- profit\n",
    "\n",
    "#### Литература:\n",
    "\n",
    "1. [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/pdf/1506.01057.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, Bidirectional, Merge\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60001, 200)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = X_train\n",
    "# X_flex = X_flex_train\n",
    "# y = y_train[:, :len(morpho_map[\"POS\"])]\n",
    "\n",
    "\n",
    "\n",
    "# Embedding dimensions.\n",
    "# tok2vec_dim = 128\n",
    "# flex2vec_dim = 32\n",
    "# char2vec_dim = 20\n",
    "\n",
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char_hidden = 128\n",
    "token_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, (1086148, 20))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ch), (X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "stem_in = Input(shape=(X_stem_train.shape[1],))\n",
    "flex_in = Input(shape=(X_flex_train.shape[1],))\n",
    "x_in = Input(shape=(X_train.shape[1],))\n",
    "\n",
    "# stem_only_in = Input(shape=(1,))\n",
    "# flex_only_in = Input(shape=(1,))\n",
    "\n",
    "stem_embedding = Embedding(input_dim=token_vecs.shape[0], \n",
    "                           output_dim=token_vecs.shape[1],\n",
    "                           weights=[token_vecs])\n",
    "# root_embedding.trainable = False\n",
    "#root_embedding_r = Reshape((X_train.shape[1], tok2vec_dim, 1))(root_embedding)\n",
    "#root_encoded_col = TimeDistributed(LSTM(token_hidden,\n",
    "#                                  dropout_U=0.1, \n",
    "#                                  dropout_W=0.1))(root_embedding_r)\n",
    "\n",
    "\n",
    "flex_embedding = Embedding(input_dim=flex_vecs.shape[0],\n",
    "                           output_dim=flex_vecs.shape[1],\n",
    "                           weights=[flex_vecs])\n",
    "# flex_embedding.trainable = False\n",
    "#flex_embedding_r = Reshape((X_flex_train.shape[1], flex2vec_dim, 1))(flex_embedding)\n",
    "#flex_encoded_col = TimeDistributed(LSTM(flex_hidden,\n",
    "#                                  dropout_U=0.1, \n",
    "#                                  dropout_W=0.1))(flex_embedding_r)\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,\n",
    "                           output_dim=char2vec_dim)\n",
    "\n",
    "\n",
    "encoded_stem = Bidirectional(LSTM(token_hidden,\n",
    "                                   dropout_U=0.1, \n",
    "                                   dropout_W=0.1))(stem_embedding(stem_in))\n",
    "\n",
    "\n",
    "encoded_flex = Bidirectional(LSTM(flex_hidden,\n",
    "                                   dropout_U=0.1, \n",
    "                                   dropout_W=0.1))(flex_embedding(flex_in))                \n",
    "\n",
    "\n",
    "encoded_x = Bidirectional(LSTM(char_hidden,\n",
    "                                   dropout_U=0.1, \n",
    "                                   dropout_W=0.1))(char_embedding(x_in))    \n",
    "\n",
    "\n",
    "# stem_vec = keras.layers.Flatten()(root_embedding(stem_only_in))\n",
    "# flex_vec = keras.layers.Flatten()(flex_embedding(flex_only_in))\n",
    "# x_vec = keras.layers.Flatten()(flex_embedding(flex_only_in))\n",
    "\n",
    "\n",
    "merge_encoded = keras.layers.merge([encoded_stem, \n",
    "                                    encoded_flex,\n",
    "                                    encoded_x],\n",
    "                                    mode='concat')\n",
    "\n",
    "\n",
    "drop = Dropout(0.25)(merge_encoded)\n",
    "output = Dense(output_dim=400, activation='softmax')(drop)\n",
    "dropped_output = Dropout(0.5)(output)\n",
    "# prediction = Dense(output_dim=num_classes, activation='softmax')(merge_encoded)\n",
    "# pos_out = Dense(output_dim=morpho_map['POS'], activation='softmax')(dropped_output)\n",
    "# repited_hidden = RepeatVector()\n",
    "predictions = [Dense(output_dim=tag_y.shape[1], activation='softmax', name=cat)(dropped_output)\n",
    "              for cat, tag_y in zip(cat_order, y_train)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dense_84'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# morpho_map['POS']\n",
    "x = model.layers[-1]\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model = Model([stem_in, flex_in, x_in], predictions)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "             metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'input_6_ib-0'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-8bed7cc7fc65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"../models/model_4/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model_json.arch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m model_checkpoint = ModelCheckpoint(os.path.join(model_path,'weights.model'),\n\u001b[1;32m      4\u001b[0m                                    \u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                                    save_best_only=True, mode='auto')\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mto_json\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m   2867\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Not JSON Serializable:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2868\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2869\u001b[0;31m         \u001b[0mmodel_config\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_updated_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2870\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_json_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2871\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m_updated_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2840\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mkeras_version\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2842\u001b[0;31m         \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2843\u001b[0m         model_config = {\n\u001b[1;32m   2844\u001b[0m             \u001b[0;34m'class_name'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mget_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2519\u001b[0m             \u001b[0mnode_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layers_node_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2520\u001b[0m             \u001b[0mnode_key\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ib-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2521\u001b[0;31m             \u001b[0mnew_node_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnode_conversion_map\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnode_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2522\u001b[0m             \u001b[0mtensor_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layers_tensor_indices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2523\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_node_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'input_6_ib-0'"
     ]
    }
   ],
   "source": [
    "model_path = \"../models/model_4/\"\n",
    "open(os.path.join(model_path, 'model_json.arch'), 'w' ).write(model.to_json())\n",
    "model_checkpoint = ModelCheckpoint(os.path.join(model_path,'weights.model'),\n",
    "                                   monitor='val_loss', verbose=1,\n",
    "                                   save_best_only=True, mode='auto')\n",
    "\n",
    "early_stopping = EarlyStopping( monitor='val_loss', patience=5, verbose=1, mode='auto')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "###  model.fit  происходит здесь:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Animacy',\n",
       " 'Case',\n",
       " 'Degree',\n",
       " 'Form',\n",
       " 'Gender',\n",
       " 'Mood',\n",
       " 'Number',\n",
       " 'POS',\n",
       " 'Person',\n",
       " 'Tense',\n",
       " 'Variant',\n",
       " 'VerbForm',\n",
       " 'Voice']"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1360643 samples, validate on 102415 samples\n",
      "Epoch 1/15\n",
      "564s - loss: 7.9048 - Animacy_loss: 0.5036 - Case_loss: 1.1747 - Degree_loss: 0.4203 - Form_loss: 0.1219 - Gender_loss: 0.9080 - Mood_loss: 0.3448 - Number_loss: 0.7662 - POS_loss: 1.7803 - Person_loss: 0.4858 - Tense_loss: 0.4314 - Variant_loss: 0.1412 - VerbForm_loss: 0.4640 - Voice_loss: 0.3625 - Animacy_acc: 0.8305 - Case_acc: 0.6250 - Degree_acc: 0.8802 - Form_acc: 0.9918 - Gender_acc: 0.6571 - Mood_acc: 0.9170 - Number_acc: 0.6637 - POS_acc: 0.4382 - Person_acc: 0.8896 - Tense_acc: 0.9028 - Variant_acc: 0.9860 - VerbForm_acc: 0.8918 - Voice_acc: 0.9083 - val_loss: 4.0095 - val_Animacy_loss: 0.2227 - val_Case_loss: 0.7628 - val_Degree_loss: 0.1569 - val_Form_loss: 0.0688 - val_Gender_loss: 0.6095 - val_Mood_loss: 0.1104 - val_Number_loss: 0.4256 - val_POS_loss: 0.9201 - val_Person_loss: 0.2916 - val_Tense_loss: 0.1533 - val_Variant_loss: 0.0529 - val_VerbForm_loss: 0.1153 - val_Voice_loss: 0.1196 - val_Animacy_acc: 0.9318 - val_Case_acc: 0.7402 - val_Degree_acc: 0.9551 - val_Form_acc: 0.9840 - val_Gender_acc: 0.7396 - val_Mood_acc: 0.9789 - val_Number_acc: 0.8418 - val_POS_acc: 0.7214 - val_Person_acc: 0.8669 - val_Tense_acc: 0.9255 - val_Variant_acc: 0.9867 - val_VerbForm_acc: 0.9568 - val_Voice_acc: 0.9707\n",
      "Epoch 2/15\n",
      "564s - loss: 4.1291 - Animacy_loss: 0.2230 - Case_loss: 0.8107 - Degree_loss: 0.1840 - Form_loss: 0.0267 - Gender_loss: 0.6628 - Mood_loss: 0.1046 - Number_loss: 0.4670 - POS_loss: 0.9049 - Person_loss: 0.2503 - Tense_loss: 0.1623 - Variant_loss: 0.0525 - VerbForm_loss: 0.1405 - Voice_loss: 0.1398 - Animacy_acc: 0.9302 - Case_acc: 0.7034 - Degree_acc: 0.9366 - Form_acc: 0.9929 - Gender_acc: 0.7249 - Mood_acc: 0.9676 - Number_acc: 0.8096 - POS_acc: 0.7252 - Person_acc: 0.9132 - Tense_acc: 0.9384 - Variant_acc: 0.9865 - VerbForm_acc: 0.9586 - Voice_acc: 0.9513 - val_loss: 2.6503 - val_Animacy_loss: 0.1583 - val_Case_loss: 0.6427 - val_Degree_loss: 0.0891 - val_Form_loss: 0.0202 - val_Gender_loss: 0.4884 - val_Mood_loss: 0.0608 - val_Number_loss: 0.3185 - val_POS_loss: 0.4374 - val_Person_loss: 0.1882 - val_Tense_loss: 0.0833 - val_Variant_loss: 0.0406 - val_VerbForm_loss: 0.0541 - val_Voice_loss: 0.0686 - val_Animacy_acc: 0.9404 - val_Case_acc: 0.7607 - val_Degree_acc: 0.9780 - val_Form_acc: 0.9840 - val_Gender_acc: 0.7907 - val_Mood_acc: 0.9822 - val_Number_acc: 0.8708 - val_POS_acc: 0.9229 - val_Person_acc: 0.9369 - val_Tense_acc: 0.9801 - val_Variant_acc: 0.9867 - val_VerbForm_acc: 0.9881 - val_Voice_acc: 0.9736\n",
      "Epoch 3/15\n",
      "564s - loss: 2.8183 - Animacy_loss: 0.1433 - Case_loss: 0.6929 - Degree_loss: 0.1109 - Form_loss: 0.0142 - Gender_loss: 0.5057 - Mood_loss: 0.0626 - Number_loss: 0.3142 - POS_loss: 0.4899 - Person_loss: 0.1869 - Tense_loss: 0.0980 - Variant_loss: 0.0364 - VerbForm_loss: 0.0716 - Voice_loss: 0.0917 - Animacy_acc: 0.9535 - Case_acc: 0.7394 - Degree_acc: 0.9647 - Form_acc: 0.9965 - Gender_acc: 0.7958 - Mood_acc: 0.9805 - Number_acc: 0.8762 - POS_acc: 0.8779 - Person_acc: 0.9328 - Tense_acc: 0.9641 - Variant_acc: 0.9894 - VerbForm_acc: 0.9795 - Voice_acc: 0.9655 - val_loss: 1.8820 - val_Animacy_loss: 0.1113 - val_Case_loss: 0.5526 - val_Degree_loss: 0.0722 - val_Form_loss: 0.0074 - val_Gender_loss: 0.3343 - val_Mood_loss: 0.0494 - val_Number_loss: 0.2057 - val_POS_loss: 0.2320 - val_Person_loss: 0.1476 - val_Tense_loss: 0.0541 - val_Variant_loss: 0.0235 - val_VerbForm_loss: 0.0381 - val_Voice_loss: 0.0540 - val_Animacy_acc: 0.9655 - val_Case_acc: 0.8017 - val_Degree_acc: 0.9801 - val_Form_acc: 0.9989 - val_Gender_acc: 0.8956 - val_Mood_acc: 0.9833 - val_Number_acc: 0.9212 - val_POS_acc: 0.9544 - val_Person_acc: 0.9391 - val_Tense_acc: 0.9848 - val_Variant_acc: 0.9914 - val_VerbForm_acc: 0.9901 - val_Voice_acc: 0.9761\n",
      "Epoch 4/15\n",
      "564s - loss: 2.1517 - Animacy_loss: 0.1190 - Case_loss: 0.5933 - Degree_loss: 0.0871 - Form_loss: 0.0122 - Gender_loss: 0.3723 - Mood_loss: 0.0471 - Number_loss: 0.2363 - POS_loss: 0.3242 - Person_loss: 0.1415 - Tense_loss: 0.0703 - Variant_loss: 0.0274 - VerbForm_loss: 0.0514 - Voice_loss: 0.0695 - Animacy_acc: 0.9623 - Case_acc: 0.7808 - Degree_acc: 0.9729 - Form_acc: 0.9973 - Gender_acc: 0.8620 - Mood_acc: 0.9861 - Number_acc: 0.9103 - POS_acc: 0.9251 - Person_acc: 0.9491 - Tense_acc: 0.9747 - Variant_acc: 0.9920 - VerbForm_acc: 0.9866 - Voice_acc: 0.9754 - val_loss: 1.4700 - val_Animacy_loss: 0.1033 - val_Case_loss: 0.4503 - val_Degree_loss: 0.0609 - val_Form_loss: 0.0077 - val_Gender_loss: 0.2248 - val_Mood_loss: 0.0405 - val_Number_loss: 0.1656 - val_POS_loss: 0.1888 - val_Person_loss: 0.0920 - val_Tense_loss: 0.0425 - val_Variant_loss: 0.0165 - val_VerbForm_loss: 0.0355 - val_Voice_loss: 0.0417 - val_Animacy_acc: 0.9703 - val_Case_acc: 0.8537 - val_Degree_acc: 0.9811 - val_Form_acc: 0.9989 - val_Gender_acc: 0.9393 - val_Mood_acc: 0.9849 - val_Number_acc: 0.9505 - val_POS_acc: 0.9592 - val_Person_acc: 0.9751 - val_Tense_acc: 0.9889 - val_Variant_acc: 0.9948 - val_VerbForm_acc: 0.9918 - val_Voice_acc: 0.9894\n",
      "Epoch 5/15\n",
      "565s - loss: 1.7105 - Animacy_loss: 0.1057 - Case_loss: 0.4842 - Degree_loss: 0.0709 - Form_loss: 0.0085 - Gender_loss: 0.2834 - Mood_loss: 0.0366 - Number_loss: 0.1935 - POS_loss: 0.2524 - Person_loss: 0.1024 - Tense_loss: 0.0532 - Variant_loss: 0.0222 - VerbForm_loss: 0.0409 - Voice_loss: 0.0565 - Animacy_acc: 0.9675 - Case_acc: 0.8278 - Degree_acc: 0.9787 - Form_acc: 0.9975 - Gender_acc: 0.9026 - Mood_acc: 0.9902 - Number_acc: 0.9298 - POS_acc: 0.9425 - Person_acc: 0.9651 - Tense_acc: 0.9823 - Variant_acc: 0.9931 - VerbForm_acc: 0.9901 - Voice_acc: 0.9804 - val_loss: 1.1874 - val_Animacy_loss: 0.0962 - val_Case_loss: 0.3450 - val_Degree_loss: 0.0525 - val_Form_loss: 0.0062 - val_Gender_loss: 0.1683 - val_Mood_loss: 0.0343 - val_Number_loss: 0.1321 - val_POS_loss: 0.1688 - val_Person_loss: 0.0627 - val_Tense_loss: 0.0360 - val_Variant_loss: 0.0128 - val_VerbForm_loss: 0.0337 - val_Voice_loss: 0.0389 - val_Animacy_acc: 0.9731 - val_Case_acc: 0.8996 - val_Degree_acc: 0.9842 - val_Form_acc: 0.9990 - val_Gender_acc: 0.9577 - val_Mood_acc: 0.9914 - val_Number_acc: 0.9619 - val_POS_acc: 0.9615 - val_Person_acc: 0.9912 - val_Tense_acc: 0.9917 - val_Variant_acc: 0.9960 - val_VerbForm_acc: 0.9926 - val_Voice_acc: 0.9888\n",
      "Epoch 6/15\n",
      "565s - loss: 1.3934 - Animacy_loss: 0.0932 - Case_loss: 0.3964 - Degree_loss: 0.0609 - Form_loss: 0.0051 - Gender_loss: 0.2241 - Mood_loss: 0.0302 - Number_loss: 0.1552 - POS_loss: 0.2063 - Person_loss: 0.0784 - Tense_loss: 0.0434 - Variant_loss: 0.0185 - VerbForm_loss: 0.0346 - Voice_loss: 0.0470 - Animacy_acc: 0.9726 - Case_acc: 0.8625 - Degree_acc: 0.9825 - Form_acc: 0.9986 - Gender_acc: 0.9284 - Mood_acc: 0.9923 - Number_acc: 0.9475 - POS_acc: 0.9533 - Person_acc: 0.9750 - Tense_acc: 0.9868 - Variant_acc: 0.9943 - VerbForm_acc: 0.9919 - Voice_acc: 0.9846 - val_loss: 1.0221 - val_Animacy_loss: 0.0911 - val_Case_loss: 0.2749 - val_Degree_loss: 0.0478 - val_Form_loss: 0.0049 - val_Gender_loss: 0.1441 - val_Mood_loss: 0.0324 - val_Number_loss: 0.1108 - val_POS_loss: 0.1560 - val_Person_loss: 0.0472 - val_Tense_loss: 0.0348 - val_Variant_loss: 0.0122 - val_VerbForm_loss: 0.0324 - val_Voice_loss: 0.0335 - val_Animacy_acc: 0.9750 - val_Case_acc: 0.9228 - val_Degree_acc: 0.9869 - val_Form_acc: 0.9991 - val_Gender_acc: 0.9638 - val_Mood_acc: 0.9926 - val_Number_acc: 0.9694 - val_POS_acc: 0.9635 - val_Person_acc: 0.9930 - val_Tense_acc: 0.9917 - val_Variant_acc: 0.9962 - val_VerbForm_acc: 0.9929 - val_Voice_acc: 0.9910\n",
      "Epoch 7/15\n",
      "565s - loss: 1.1683 - Animacy_loss: 0.0824 - Case_loss: 0.3285 - Degree_loss: 0.0532 - Form_loss: 0.0041 - Gender_loss: 0.1855 - Mood_loss: 0.0260 - Number_loss: 0.1286 - POS_loss: 0.1760 - Person_loss: 0.0630 - Tense_loss: 0.0362 - Variant_loss: 0.0158 - VerbForm_loss: 0.0298 - Voice_loss: 0.0392 - Animacy_acc: 0.9768 - Case_acc: 0.8910 - Degree_acc: 0.9850 - Form_acc: 0.9990 - Gender_acc: 0.9442 - Mood_acc: 0.9935 - Number_acc: 0.9580 - POS_acc: 0.9601 - Person_acc: 0.9812 - Tense_acc: 0.9899 - Variant_acc: 0.9951 - VerbForm_acc: 0.9931 - Voice_acc: 0.9878 - val_loss: 0.9281 - val_Animacy_loss: 0.0885 - val_Case_loss: 0.2355 - val_Degree_loss: 0.0465 - val_Form_loss: 0.0051 - val_Gender_loss: 0.1317 - val_Mood_loss: 0.0296 - val_Number_loss: 0.1003 - val_POS_loss: 0.1498 - val_Person_loss: 0.0368 - val_Tense_loss: 0.0315 - val_Variant_loss: 0.0116 - val_VerbForm_loss: 0.0313 - val_Voice_loss: 0.0299 - val_Animacy_acc: 0.9759 - val_Case_acc: 0.9358 - val_Degree_acc: 0.9868 - val_Form_acc: 0.9991 - val_Gender_acc: 0.9650 - val_Mood_acc: 0.9934 - val_Number_acc: 0.9734 - val_POS_acc: 0.9655 - val_Person_acc: 0.9938 - val_Tense_acc: 0.9925 - val_Variant_acc: 0.9963 - val_VerbForm_acc: 0.9933 - val_Voice_acc: 0.9924\n",
      "Epoch 8/15\n",
      "565s - loss: 1.0118 - Animacy_loss: 0.0744 - Case_loss: 0.2796 - Degree_loss: 0.0474 - Form_loss: 0.0033 - Gender_loss: 0.1609 - Mood_loss: 0.0230 - Number_loss: 0.1123 - POS_loss: 0.1534 - Person_loss: 0.0527 - Tense_loss: 0.0316 - Variant_loss: 0.0140 - VerbForm_loss: 0.0263 - Voice_loss: 0.0330 - Animacy_acc: 0.9794 - Case_acc: 0.9110 - Degree_acc: 0.9869 - Form_acc: 0.9992 - Gender_acc: 0.9528 - Mood_acc: 0.9943 - Number_acc: 0.9643 - POS_acc: 0.9654 - Person_acc: 0.9850 - Tense_acc: 0.9914 - Variant_acc: 0.9956 - VerbForm_acc: 0.9940 - Voice_acc: 0.9900 - val_loss: 0.8762 - val_Animacy_loss: 0.0887 - val_Case_loss: 0.2071 - val_Degree_loss: 0.0469 - val_Form_loss: 0.0047 - val_Gender_loss: 0.1264 - val_Mood_loss: 0.0288 - val_Number_loss: 0.0951 - val_POS_loss: 0.1483 - val_Person_loss: 0.0324 - val_Tense_loss: 0.0298 - val_Variant_loss: 0.0098 - val_VerbForm_loss: 0.0308 - val_Voice_loss: 0.0272 - val_Animacy_acc: 0.9761 - val_Case_acc: 0.9411 - val_Degree_acc: 0.9869 - val_Form_acc: 0.9991 - val_Gender_acc: 0.9668 - val_Mood_acc: 0.9936 - val_Number_acc: 0.9743 - val_POS_acc: 0.9661 - val_Person_acc: 0.9941 - val_Tense_acc: 0.9930 - val_Variant_acc: 0.9970 - val_VerbForm_acc: 0.9934 - val_Voice_acc: 0.9937\n",
      "Epoch 9/15\n",
      "564s - loss: 0.9026 - Animacy_loss: 0.0689 - Case_loss: 0.2460 - Degree_loss: 0.0435 - Form_loss: 0.0032 - Gender_loss: 0.1444 - Mood_loss: 0.0211 - Number_loss: 0.1006 - POS_loss: 0.1366 - Person_loss: 0.0455 - Tense_loss: 0.0283 - Variant_loss: 0.0124 - VerbForm_loss: 0.0240 - Voice_loss: 0.0281 - Animacy_acc: 0.9811 - Case_acc: 0.9229 - Degree_acc: 0.9881 - Form_acc: 0.9993 - Gender_acc: 0.9581 - Mood_acc: 0.9948 - Number_acc: 0.9687 - POS_acc: 0.9692 - Person_acc: 0.9875 - Tense_acc: 0.9924 - Variant_acc: 0.9960 - VerbForm_acc: 0.9946 - Voice_acc: 0.9922 - val_loss: 0.8630 - val_Animacy_loss: 0.0906 - val_Case_loss: 0.1938 - val_Degree_loss: 0.0458 - val_Form_loss: 0.0049 - val_Gender_loss: 0.1251 - val_Mood_loss: 0.0297 - val_Number_loss: 0.0930 - val_POS_loss: 0.1484 - val_Person_loss: 0.0309 - val_Tense_loss: 0.0302 - val_Variant_loss: 0.0110 - val_VerbForm_loss: 0.0323 - val_Voice_loss: 0.0274 - val_Animacy_acc: 0.9763 - val_Case_acc: 0.9460 - val_Degree_acc: 0.9877 - val_Form_acc: 0.9992 - val_Gender_acc: 0.9675 - val_Mood_acc: 0.9936 - val_Number_acc: 0.9754 - val_POS_acc: 0.9670 - val_Person_acc: 0.9941 - val_Tense_acc: 0.9934 - val_Variant_acc: 0.9968 - val_VerbForm_acc: 0.9935 - val_Voice_acc: 0.9940\n",
      "Epoch 10/15\n",
      "564s - loss: 0.8223 - Animacy_loss: 0.0644 - Case_loss: 0.2231 - Degree_loss: 0.0404 - Form_loss: 0.0029 - Gender_loss: 0.1329 - Mood_loss: 0.0192 - Number_loss: 0.0924 - POS_loss: 0.1245 - Person_loss: 0.0395 - Tense_loss: 0.0253 - Variant_loss: 0.0112 - VerbForm_loss: 0.0218 - Voice_loss: 0.0245 - Animacy_acc: 0.9823 - Case_acc: 0.9304 - Degree_acc: 0.9890 - Form_acc: 0.9994 - Gender_acc: 0.9618 - Mood_acc: 0.9952 - Number_acc: 0.9714 - POS_acc: 0.9718 - Person_acc: 0.9896 - Tense_acc: 0.9934 - Variant_acc: 0.9965 - VerbForm_acc: 0.9951 - Voice_acc: 0.9935 - val_loss: 0.8217 - val_Animacy_loss: 0.0876 - val_Case_loss: 0.1836 - val_Degree_loss: 0.0440 - val_Form_loss: 0.0045 - val_Gender_loss: 0.1191 - val_Mood_loss: 0.0276 - val_Number_loss: 0.0880 - val_POS_loss: 0.1431 - val_Person_loss: 0.0285 - val_Tense_loss: 0.0286 - val_Variant_loss: 0.0100 - val_VerbForm_loss: 0.0308 - val_Voice_loss: 0.0262 - val_Animacy_acc: 0.9767 - val_Case_acc: 0.9478 - val_Degree_acc: 0.9879 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9682 - val_Mood_acc: 0.9938 - val_Number_acc: 0.9764 - val_POS_acc: 0.9676 - val_Person_acc: 0.9944 - val_Tense_acc: 0.9934 - val_Variant_acc: 0.9971 - val_VerbForm_acc: 0.9934 - val_Voice_acc: 0.9937\n",
      "Epoch 11/15\n",
      "564s - loss: 0.7547 - Animacy_loss: 0.0607 - Case_loss: 0.2041 - Degree_loss: 0.0375 - Form_loss: 0.0027 - Gender_loss: 0.1227 - Mood_loss: 0.0177 - Number_loss: 0.0846 - POS_loss: 0.1150 - Person_loss: 0.0350 - Tense_loss: 0.0230 - Variant_loss: 0.0103 - VerbForm_loss: 0.0198 - Voice_loss: 0.0217 - Animacy_acc: 0.9835 - Case_acc: 0.9370 - Degree_acc: 0.9898 - Form_acc: 0.9994 - Gender_acc: 0.9651 - Mood_acc: 0.9956 - Number_acc: 0.9742 - POS_acc: 0.9738 - Person_acc: 0.9910 - Tense_acc: 0.9941 - Variant_acc: 0.9968 - VerbForm_acc: 0.9956 - Voice_acc: 0.9944 - val_loss: 0.8114 - val_Animacy_loss: 0.0873 - val_Case_loss: 0.1766 - val_Degree_loss: 0.0455 - val_Form_loss: 0.0045 - val_Gender_loss: 0.1193 - val_Mood_loss: 0.0271 - val_Number_loss: 0.0876 - val_POS_loss: 0.1421 - val_Person_loss: 0.0278 - val_Tense_loss: 0.0280 - val_Variant_loss: 0.0099 - val_VerbForm_loss: 0.0303 - val_Voice_loss: 0.0255 - val_Animacy_acc: 0.9773 - val_Case_acc: 0.9499 - val_Degree_acc: 0.9878 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9686 - val_Mood_acc: 0.9938 - val_Number_acc: 0.9765 - val_POS_acc: 0.9684 - val_Person_acc: 0.9944 - val_Tense_acc: 0.9937 - val_Variant_acc: 0.9972 - val_VerbForm_acc: 0.9934 - val_Voice_acc: 0.9938\n",
      "Epoch 12/15\n",
      "564s - loss: 0.7067 - Animacy_loss: 0.0574 - Case_loss: 0.1916 - Degree_loss: 0.0350 - Form_loss: 0.0026 - Gender_loss: 0.1154 - Mood_loss: 0.0167 - Number_loss: 0.0797 - POS_loss: 0.1070 - Person_loss: 0.0321 - Tense_loss: 0.0211 - Variant_loss: 0.0094 - VerbForm_loss: 0.0188 - Voice_loss: 0.0199 - Animacy_acc: 0.9844 - Case_acc: 0.9409 - Degree_acc: 0.9904 - Form_acc: 0.9995 - Gender_acc: 0.9673 - Mood_acc: 0.9958 - Number_acc: 0.9759 - POS_acc: 0.9754 - Person_acc: 0.9919 - Tense_acc: 0.9946 - Variant_acc: 0.9971 - VerbForm_acc: 0.9958 - Voice_acc: 0.9949 - val_loss: 0.8328 - val_Animacy_loss: 0.0917 - val_Case_loss: 0.1772 - val_Degree_loss: 0.0466 - val_Form_loss: 0.0049 - val_Gender_loss: 0.1221 - val_Mood_loss: 0.0280 - val_Number_loss: 0.0895 - val_POS_loss: 0.1482 - val_Person_loss: 0.0273 - val_Tense_loss: 0.0289 - val_Variant_loss: 0.0102 - val_VerbForm_loss: 0.0320 - val_Voice_loss: 0.0263 - val_Animacy_acc: 0.9774 - val_Case_acc: 0.9499 - val_Degree_acc: 0.9881 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9692 - val_Mood_acc: 0.9939 - val_Number_acc: 0.9770 - val_POS_acc: 0.9685 - val_Person_acc: 0.9946 - val_Tense_acc: 0.9943 - val_Variant_acc: 0.9970 - val_VerbForm_acc: 0.9937 - val_Voice_acc: 0.9942\n",
      "Epoch 13/15\n",
      "564s - loss: 0.6675 - Animacy_loss: 0.0550 - Case_loss: 0.1800 - Degree_loss: 0.0338 - Form_loss: 0.0025 - Gender_loss: 0.1092 - Mood_loss: 0.0157 - Number_loss: 0.0753 - POS_loss: 0.1020 - Person_loss: 0.0294 - Tense_loss: 0.0196 - Variant_loss: 0.0089 - VerbForm_loss: 0.0176 - Voice_loss: 0.0185 - Animacy_acc: 0.9853 - Case_acc: 0.9450 - Degree_acc: 0.9908 - Form_acc: 0.9995 - Gender_acc: 0.9696 - Mood_acc: 0.9961 - Number_acc: 0.9773 - POS_acc: 0.9768 - Person_acc: 0.9927 - Tense_acc: 0.9950 - Variant_acc: 0.9972 - VerbForm_acc: 0.9961 - Voice_acc: 0.9953 - val_loss: 0.8320 - val_Animacy_loss: 0.0915 - val_Case_loss: 0.1735 - val_Degree_loss: 0.0478 - val_Form_loss: 0.0045 - val_Gender_loss: 0.1219 - val_Mood_loss: 0.0280 - val_Number_loss: 0.0899 - val_POS_loss: 0.1490 - val_Person_loss: 0.0279 - val_Tense_loss: 0.0286 - val_Variant_loss: 0.0107 - val_VerbForm_loss: 0.0321 - val_Voice_loss: 0.0264 - val_Animacy_acc: 0.9777 - val_Case_acc: 0.9507 - val_Degree_acc: 0.9876 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9689 - val_Mood_acc: 0.9941 - val_Number_acc: 0.9768 - val_POS_acc: 0.9682 - val_Person_acc: 0.9946 - val_Tense_acc: 0.9946 - val_Variant_acc: 0.9971 - val_VerbForm_acc: 0.9938 - val_Voice_acc: 0.9946\n",
      "Epoch 14/15\n",
      "564s - loss: 0.6293 - Animacy_loss: 0.0525 - Case_loss: 0.1702 - Degree_loss: 0.0318 - Form_loss: 0.0025 - Gender_loss: 0.1028 - Mood_loss: 0.0149 - Number_loss: 0.0714 - POS_loss: 0.0966 - Person_loss: 0.0268 - Tense_loss: 0.0182 - Variant_loss: 0.0081 - VerbForm_loss: 0.0165 - Voice_loss: 0.0169 - Animacy_acc: 0.9859 - Case_acc: 0.9484 - Degree_acc: 0.9913 - Form_acc: 0.9995 - Gender_acc: 0.9715 - Mood_acc: 0.9963 - Number_acc: 0.9787 - POS_acc: 0.9776 - Person_acc: 0.9934 - Tense_acc: 0.9954 - Variant_acc: 0.9975 - VerbForm_acc: 0.9963 - Voice_acc: 0.9959 - val_loss: 0.8152 - val_Animacy_loss: 0.0897 - val_Case_loss: 0.1704 - val_Degree_loss: 0.0468 - val_Form_loss: 0.0045 - val_Gender_loss: 0.1207 - val_Mood_loss: 0.0270 - val_Number_loss: 0.0894 - val_POS_loss: 0.1457 - val_Person_loss: 0.0276 - val_Tense_loss: 0.0272 - val_Variant_loss: 0.0096 - val_VerbForm_loss: 0.0311 - val_Voice_loss: 0.0255 - val_Animacy_acc: 0.9778 - val_Case_acc: 0.9519 - val_Degree_acc: 0.9881 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9697 - val_Mood_acc: 0.9942 - val_Number_acc: 0.9776 - val_POS_acc: 0.9690 - val_Person_acc: 0.9946 - val_Tense_acc: 0.9946 - val_Variant_acc: 0.9971 - val_VerbForm_acc: 0.9938 - val_Voice_acc: 0.9946\n",
      "Epoch 15/15\n",
      "564s - loss: 0.6005 - Animacy_loss: 0.0507 - Case_loss: 0.1614 - Degree_loss: 0.0306 - Form_loss: 0.0023 - Gender_loss: 0.0981 - Mood_loss: 0.0143 - Number_loss: 0.0682 - POS_loss: 0.0927 - Person_loss: 0.0253 - Tense_loss: 0.0173 - Variant_loss: 0.0077 - VerbForm_loss: 0.0160 - Voice_loss: 0.0159 - Animacy_acc: 0.9864 - Case_acc: 0.9513 - Degree_acc: 0.9916 - Form_acc: 0.9995 - Gender_acc: 0.9731 - Mood_acc: 0.9965 - Number_acc: 0.9800 - POS_acc: 0.9785 - Person_acc: 0.9939 - Tense_acc: 0.9957 - Variant_acc: 0.9976 - VerbForm_acc: 0.9965 - Voice_acc: 0.9962 - val_loss: 0.8368 - val_Animacy_loss: 0.0939 - val_Case_loss: 0.1721 - val_Degree_loss: 0.0475 - val_Form_loss: 0.0046 - val_Gender_loss: 0.1218 - val_Mood_loss: 0.0286 - val_Number_loss: 0.0911 - val_POS_loss: 0.1502 - val_Person_loss: 0.0285 - val_Tense_loss: 0.0285 - val_Variant_loss: 0.0097 - val_VerbForm_loss: 0.0330 - val_Voice_loss: 0.0273 - val_Animacy_acc: 0.9776 - val_Case_acc: 0.9517 - val_Degree_acc: 0.9880 - val_Form_acc: 0.9993 - val_Gender_acc: 0.9700 - val_Mood_acc: 0.9941 - val_Number_acc: 0.9775 - val_POS_acc: 0.9694 - val_Person_acc: 0.9945 - val_Tense_acc: 0.9946 - val_Variant_acc: 0.9973 - val_VerbForm_acc: 0.9937 - val_Voice_acc: 0.9944\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f68faa620f0>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 15\n",
    "model.fit([X_stem, X_flex, X], y, validation_split=0.07,\n",
    "          #validation_data=([X_stem_test, X_flex_test, X_test], y_test),\n",
    "          batch_size=batch_size, nb_epoch=epochs, \n",
    "          verbose=2) #, callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7f6992b77080>,\n",
       " <keras.engine.topology.InputLayer at 0x7f6992b77048>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f6992b77320>,\n",
       " <keras.layers.embeddings.Embedding at 0x7f6992b77518>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7f6992b77438>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7f69918541d0>,\n",
       " <keras.engine.topology.Merge at 0x7f69931c3518>,\n",
       " <keras.layers.core.Dropout at 0x7f6991845630>,\n",
       " <keras.layers.core.Dense at 0x7f6997d55860>,\n",
       " <keras.layers.core.Dropout at 0x7f6997cfbc88>,\n",
       " <keras.layers.core.Dense at 0x7f6997de80b8>,\n",
       " <keras.layers.core.Dense at 0x7f6997e0fba8>,\n",
       " <keras.layers.core.Dense at 0x7f6997e0a710>,\n",
       " <keras.layers.core.Dense at 0x7f6997e06550>,\n",
       " <keras.layers.core.Dense at 0x7f6997e13668>,\n",
       " <keras.layers.core.Dense at 0x7f6997dfddd8>,\n",
       " <keras.layers.core.Dense at 0x7f6997df9978>,\n",
       " <keras.layers.core.Dense at 0x7f6997e18da0>,\n",
       " <keras.layers.core.Dense at 0x7f6a1be500b8>,\n",
       " <keras.layers.core.Dense at 0x7f6a1d5b68d0>,\n",
       " <keras.layers.core.Dense at 0x7f6a1d4f5f98>,\n",
       " <keras.layers.core.Dense at 0x7f6a1b9ff240>,\n",
       " <keras.layers.core.Dense at 0x7f6992b7c898>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_checkpoint' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-79-90285ac9f049>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m           \u001b[0;31m# validation_data=([X_stem_test, X_flex_test, X_test], y_test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m           verbose=2, callbacks=[model_checkpoint, early_stopping])\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model_checkpoint' is not defined"
     ]
    }
   ],
   "source": [
    "batch_size = 512\n",
    "epochs = 1\n",
    "model.fit([X_stem, X_flex, X], y, validation_split=0.05,\n",
    "          # validation_data=([X_stem_test, X_flex_test, X_test], y_test)\n",
    "          batch_size=batch_size, nb_epoch=epochs,\n",
    "          verbose=2, callbacks=[model_checkpoint, early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.topology.InputLayer at 0x7fe1e6af8588>,\n",
       " <keras.engine.topology.InputLayer at 0x7fe1e6af8550>,\n",
       " <keras.engine.topology.InputLayer at 0x7fe1e6af8748>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fe1e6af8f28>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fe1e5f94160>,\n",
       " <keras.layers.embeddings.Embedding at 0x7fe1e5f94198>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7fe1e5f94358>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7fe16d10a438>,\n",
       " <keras.layers.wrappers.Bidirectional at 0x7fe16cf58470>,\n",
       " <keras.engine.topology.Merge at 0x7fe16d1110b8>,\n",
       " <keras.layers.core.Dropout at 0x7fe16cf58780>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccab1d0>,\n",
       " <keras.layers.core.Dropout at 0x7fe16ccab748>,\n",
       " <keras.layers.core.Dense at 0x7fe16cca49b0>,\n",
       " <keras.layers.core.Dense at 0x7fe16cccec88>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccd28d0>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccc28d0>,\n",
       " <keras.layers.core.Dense at 0x7fe16cca4e80>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccfcf60>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccfc358>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccf4eb8>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccf0630>,\n",
       " <keras.layers.core.Dense at 0x7fe16ccf0208>,\n",
       " <keras.layers.core.Dense at 0x7fe16cd5ecf8>,\n",
       " <keras.layers.core.Dense at 0x7fe16cd5dcc0>,\n",
       " <keras.layers.core.Dense at 0x7fe16cd5d5f8>]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: expected input_4 to have shape (None, 11) but got array with shape (815884, 20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-36-ae819a1a1c1e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model.fit([X_train, X_flex_train, X_train], y_train, validation_split=0.1,\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m           verbose=2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1066\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1067\u001b[0m             \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1068\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1069\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_dim, batch_size)\u001b[0m\n\u001b[1;32m    979\u001b[0m                                    \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    980\u001b[0m                                    \u001b[0mcheck_batch_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 981\u001b[0;31m                                    exception_prefix='model input')\n\u001b[0m\u001b[1;32m    982\u001b[0m         y = standardize_input_data(y, self.output_names,\n\u001b[1;32m    983\u001b[0m                                    \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_dim, exception_prefix)\u001b[0m\n\u001b[1;32m    111\u001b[0m                             \u001b[0;34m' to have shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshapes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m                             \u001b[0;34m' but got array with shape '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                             str(array.shape))\n\u001b[0m\u001b[1;32m    114\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: expected input_4 to have shape (None, 11) but got array with shape (815884, 20)"
     ]
    }
   ],
   "source": [
    "model.fit([X_train, X_flex_train, X_train], y_train, validation_split=0.1,\n",
    "          batch_size=batch_size, nb_epoch=epochs,\n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model_path = \"\"\n",
    "json_string = model.to_json()\n",
    "open(os.path.join(model_path, \"json_model\"), \"w\").write(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "one_hots = many_probs_to_one_hot(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  1.74051713e-12,   9.99907136e-01,   9.10107519e-06,\n",
       "          5.71419914e-05,   1.72988200e-06,   5.51274637e-10,\n",
       "          1.09171277e-12,   4.06685899e-07,   3.89459203e-08,\n",
       "          2.32082493e-05,   1.72790578e-07,   1.04286542e-07,\n",
       "          7.78666674e-07,   2.22505481e-09], dtype=float32),\n",
       " array([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]))"
      ]
     },
     "execution_count": 544,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted[7][0], one_hots[7][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.97397359618743151"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(pred_POS ==  y.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### SOME RESULTS:\n",
    "\n",
    "#### первые шаги:\n",
    "gikrya_train, вход: флексии + стеммы, выход: POS, словарь токенов max_size=2000, словарь флексий max_size=500, val_split=0.1, промежуточный результат:\n",
    "\n",
    ">Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    ">Epoch 1/5: 2436s - loss: 0.3060 - acc: 0.9024 - val_loss: 0.1234 - val_acc: 0.9630\n",
    "\n",
    ">Epoch 2/5: 2407s - loss: 0.1219 - acc: 0.9637 - val_loss: 0.1003 - val_acc: 0.9687\n",
    "\n",
    ">Epoch 3/5: 2390s - loss: 0.1023 - acc: 0.9694 - val_loss: 0.0910 - val_acc: 0.9727\n",
    "\n",
    ">Epoch 4/5: 2401s - loss: 0.0938 - acc: 0.9724 - val_loss: 0.0852 - val_acc: 0.9752\n",
    "\n",
    "> Epoch 5/5: 2397s - loss: 0.0888 - acc: 0.9744 - val_loss: 0.0853 - val_acc: 0.9764\n",
    "\n",
    "+\n",
    "\n",
    ">Epoch 6: 2397s - loss: 0.0866 - acc: 0.9754 - val_loss: 0.0901 - val_acc: 0.9757\n",
    "\n",
    "#### Замена LSTM на Simple RNN\n",
    "> Epoch 2: 663s - loss: 0.1494 - acc: 0.9551 - val_loss: 0.1181 - val_acc: 0.9645\n",
    "\n",
    "> Epoch 3: 664s - loss: 0.1340 - acc: 0.9598 - val_loss: 0.1116 - val_acc: 0.9674\n",
    "\n",
    "+\n",
    "\n",
    "> Epoch 4: 663s - loss: 0.1266 - acc: 0.9624 - val_loss: 0.1075 - val_acc: 0.9672\n",
    "\n",
    "\n",
    "#### Замена GRU\n",
    "\n",
    ">Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    ">Epoch 1/2: 1922s - loss: 0.2765 - acc: 0.9138 - val_loss: 0.1233 - val_acc: 0.9627\n",
    "\n",
    ">Epoch 2/2: 1912s - loss: 0.1249 - acc: 0.9627 - val_loss: 0.0975 - val_acc: 0.9706\n",
    "\n",
    "#### GRU all outputs:\n",
    "\n",
    "Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    "> Epoch 3: \n",
    "2899s - loss: 0.7679 - dense_12_loss: 0.0605 - dense_13_loss: 0.1723 - dense_14_loss: 0.0552 - dense_15_loss: 0.0025 - dense_16_loss: 0.1316 - dense_17_loss: 0.0200 - dense_18_loss: 0.1102 - dense_19_loss: 0.1123 - dense_20_loss: 0.0242 - dense_21_loss: 0.0242 - dense_22_loss: 0.0117 - dense_23_loss: 0.0228 - dense_24_loss: 0.0203 - dense_12_acc: 0.9812 - dense_13_acc: 0.9423 - dense_14_acc: 0.9819 - dense_15_acc: 0.9993 - dense_16_acc: 0.9582 - dense_17_acc: 0.9940 - dense_18_acc: 0.9623 - dense_19_acc: 0.9659 - dense_20_acc: 0.9926 - dense_21_acc: 0.9925 - dense_22_acc: 0.9964 - dense_23_acc: 0.9932 - dense_24_acc: 0.9938 - val_loss: 0.6724 - val_dense_12_loss: 0.0527 - val_dense_13_loss: 0.1537 - val_dense_14_loss: 0.0475 - val_dense_15_loss: 0.0019 - val_dense_16_loss: 0.1114 - val_dense_17_loss: 0.0184 - val_dense_18_loss: 0.1004 - val_dense_19_loss: 0.0976 - val_dense_20_loss: 0.0211 - val_dense_21_loss: 0.0209 - val_dense_22_loss: 0.0090 - val_dense_23_loss: 0.0206 - val_dense_24_loss: 0.0173 - val_dense_12_acc: 0.9832 - val_dense_13_acc: 0.9469 - val_dense_14_acc: 0.9844 - val_dense_15_acc: 0.9995 - val_dense_16_acc: 0.9650 - val_dense_17_acc: 0.9945 - val_dense_18_acc: 0.9670 - val_dense_19_acc: 0.9697 - val_dense_20_acc: 0.9943 - val_dense_21_acc: 0.9937 - val_dense_22_acc: 0.9974 - val_dense_23_acc: 0.9941 - val_dense_24_acc: 0.9948\n",
    "\n",
    ">Epoch 4:\n",
    "2892s - loss: 0.7051 - dense_12_loss: 0.0549 - dense_13_loss: 0.1598 - dense_14_loss: 0.0513 - dense_15_loss: 0.0022 - dense_16_loss: 0.1193 - dense_17_loss: 0.0182 - dense_18_loss: 0.1010 - dense_19_loss: 0.1039 - dense_20_loss: 0.0221 - dense_21_loss: 0.0222 - dense_22_loss: 0.0109 - dense_23_loss: 0.0208 - dense_24_loss: 0.0184 - dense_12_acc: 0.9832 - dense_13_acc: 0.9467 - dense_14_acc: 0.9833 - dense_15_acc: 0.9994 - dense_16_acc: 0.9626 - dense_17_acc: 0.9946 - dense_18_acc: 0.9658 - dense_19_acc: 0.9685 - dense_20_acc: 0.9933 - dense_21_acc: 0.9933 - dense_22_acc: 0.9967 - dense_23_acc: 0.9940 - dense_24_acc: 0.9946 - val_loss: 0.6327 - val_dense_12_loss: 0.0487 - val_dense_13_loss: 0.1437 - val_dense_14_loss: 0.0461 - val_dense_15_loss: 0.0016 - val_dense_16_loss: 0.1043 - val_dense_17_loss: 0.0196 - val_dense_18_loss: 0.0904 - val_dense_19_loss: 0.0913 - val_dense_20_loss: 0.0180 - val_dense_21_loss: 0.0209 - val_dense_22_loss: 0.0085 - val_dense_23_loss: 0.0214 - val_dense_24_loss: 0.0182 - val_dense_12_acc: 0.9846 - val_dense_13_acc: 0.9505 - val_dense_14_acc: 0.9841 - val_dense_15_acc: 0.9996 - val_dense_16_acc: 0.9680 - val_dense_17_acc: 0.9944 - val_dense_18_acc: 0.9700 - val_dense_19_acc: 0.9720 - val_dense_20_acc: 0.9951 - val_dense_21_acc: 0.9938 - val_dense_22_acc: 0.9974 - val_dense_23_acc: 0.9941 - val_dense_24_acc: 0.9947\n",
    "\n",
    "\n",
    "eval на gikrya_test:\n",
    "\n",
    "> 149081 меток из 171550, точность 86.90%\n",
    "\n",
    "> 8454 предложений из 20787, точность 40.67%\n",
    "\n",
    "#### LSTM \n",
    "eval на gikrya_test:\n",
    "\n",
    "> 149674 меток из 171550, точность 87.25%\n",
    "\n",
    "> 8751 предложений из 20787, точность 42.10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This is an example of using Hierarchical RNN (HRNN) to classify MNIST digits.\n",
    "HRNNs can learn across multiple levels of temporal hiearchy over a complex sequence.\n",
    "Usually, the first recurrent layer of an HRNN encodes a sentence (e.g. of word vectors)\n",
    "into a  sentence vector. The second recurrent layer then encodes a sequence of\n",
    "such vectors (encoded by the first layer) into a document vector. This\n",
    "document vector is considered to preserve both the word-level and\n",
    "sentence-level structure of the context.\n",
    "# References\n",
    "    - [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/abs/1506.01057)\n",
    "        Encodes paragraphs and documents with HRNN.\n",
    "        Results have shown that HRNN outperforms standard\n",
    "        RNNs and may play some role in more sophisticated generation tasks like\n",
    "        summarization or question answering.\n",
    "    - [Hierarchical recurrent neural network for skeleton based action recognition](http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=7298714)\n",
    "        Achieved state-of-the-art results on skeleton based action recognition with 3 levels\n",
    "        of bidirectional HRNN combined with fully connected layers.\n",
    "In the below MNIST example the first LSTM layer first encodes every\n",
    "column of pixels of shape (28, 1) to a column vector of shape (128,). The second LSTM\n",
    "layer encodes then these 28 column vectors of shape (28, 128) to a image vector\n",
    "representing the whole image. A final Dense layer is added for prediction.\n",
    "After 5 epochs: train acc: 0.9858, val acc: 0.9864"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (60000, 28, 28, 1)\n",
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/engine/topology.py:368: UserWarning: The `regularizers` property of layers/models is deprecated. Regularization losses are now managed via the `losses` layer/model property.\n",
      "  warnings.warn('The `regularizers` property of '\n"
     ]
    }
   ],
   "source": [
    "# # Training parameters.\n",
    "# batch_size = 32\n",
    "# num_classes = 10\n",
    "# epochs = 5\n",
    "\n",
    "# # Embedding dimensions.\n",
    "# row_hidden = 128\n",
    "# col_hidden = 129\n",
    "\n",
    "# # The data, shuffled and split between train and test sets.\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()#\"/home/users1/keras_datasets/\")\n",
    "\n",
    "# # Reshapes data to 4D for Hierarchical RNN.\n",
    "# x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
    "# x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
    "# x_train = x_train.astype('float32')\n",
    "# x_test = x_test.astype('float32')\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "# print('x_train shape:', x_train.shape)\n",
    "# print(x_train.shape[0], 'train samples')\n",
    "# print(x_test.shape[0], 'test samples')\n",
    "\n",
    "# # Converts class vectors to binary class matrices.\n",
    "# y_train = keras.utils.np_utils.to_categorical(y_train, num_classes)\n",
    "# y_test = keras.utils.np_utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "# row, col, pixel = x_train.shape[1:]\n",
    "\n",
    "# # 4D input.\n",
    "# x = Input(shape=(row, col, pixel))\n",
    "\n",
    "# # Encodes a row of pixels using TimeDistributed Wrapper.\n",
    "# encoded_rows = TimeDistributed(LSTM(row_hidden))(x)\n",
    "\n",
    "# # Encodes columns of encoded rows.\n",
    "# encoded_columns = LSTM(col_hidden)(encoded_rows)\n",
    "\n",
    "# Final predictions and model.\n",
    "# prediction = Dense(num_classes, activation='softmax')(encoded_columns)\n",
    "# model = Model(x, prediction)\n",
    "# model.compile(loss='categorical_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# Training.\n",
    "# model.fit(x_train, y_train,\n",
    "#           batch_size=batch_size, nb_epoch=epochs,\n",
    "#           verbose=2, validation_data=(x_test, y_test))\n",
    "\n",
    "# # Evaluation.\n",
    "# scores = model.evaluate(x_test, y_test, verbose=0)\n",
    "# print('Test loss:', scores[0])\n",
    "# print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([Dimension(None), Dimension(28), Dimension(128)]),\n",
       " TensorShape([Dimension(None), Dimension(129)]))"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 4]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
