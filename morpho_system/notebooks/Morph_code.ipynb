{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Витин код для Диалоговской морфологии (последовательный и работающий...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from nltk.stem import SnowballStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "from gensim import corpora\n",
    "from collections import defaultdict\n",
    "import pickle\n",
    "import pprint\n",
    "from __future__ import print_function\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, TimeDistributed, Embedding, Bidirectional, Merge\n",
    "from keras.layers import LSTM, SimpleRNN, GRU, Dropout, RepeatVector\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import model_from_json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Сперва подготовим все для обучения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) pickle со словарем всех категорий и их id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNDEFINED = \"_\"\n",
    "MAX_WORD_LENGTH = 20\n",
    "\n",
    "def read_gikrya(path):\n",
    "    \"\"\"\n",
    "    Читает гикря трайн/тест файл, вытаскивает всю инфу \n",
    "    записывает в словарик все категории и их значения (+ индексы для них генерируются)\n",
    "    и предложения собирает\n",
    "    Reading format:\n",
    "    row_index<TAB>form<TAB>lemma<TAB>POS<TAB>tag\n",
    "    return sentences: array of sentence arrays. each sentence array has this structure:\n",
    "           [('рука', # token\n",
    "           'рука', # lemma\n",
    "           [('POS', 'NOUN'), # Gramms\n",
    "            ('Animacy', 'Inan'),\n",
    "            ('Case', 'Nom'),\n",
    "            ('Gender', 'Fem'),\n",
    "            ('Number', 'Sing')]), () ... () ]\n",
    "    morpho_map: { ... \n",
    "    'Number': {0: '_', 1: 'Sing', 2: 'Plur', 'Plur': 2, 'Sing': 1, '_': 0}, ... }\n",
    "    \"\"\"\n",
    "    \n",
    "    morpho_map = {\"POS\":{UNDEFINED: 0, \n",
    "                         0: UNDEFINED}}\n",
    "    sentences = []\n",
    "    vocab = {}    \n",
    "    with open(path, 'r') as f:\n",
    "        \n",
    "        sentence = []\n",
    "        for line in f:\n",
    "            splits = line.strip().split('\\t')      \n",
    "            if len(splits) == 4:\n",
    "                splits.insert(0, 1)\n",
    "            if len(splits) == 5:\n",
    "                form, lemma, POS, tags = splits[1:]\n",
    "                if POS not in  morpho_map[\"POS\"]:\n",
    "                    morpho_map[\"POS\"][POS] = len(morpho_map[\"POS\"]) // 2 \n",
    "                    morpho_map[\"POS\"][morpho_map[\"POS\"][POS]] =  POS\n",
    "                tags_list = [(\"POS\", POS)]\n",
    "                if tags != \"_\":\n",
    "                    for tag_val in tags.split(\"|\"):\n",
    "                        tag, val = tag_val.split(\"=\")\n",
    "                        tags_list.append((tag, val))\n",
    "                        if tag not in morpho_map:\n",
    "                            morpho_map[tag] = {UNDEFINED: 0,\n",
    "                                               0: UNDEFINED}\n",
    "                        if val not in morpho_map[tag]:\n",
    "                            morpho_map[tag][val] = len(morpho_map[tag]) // 2 \n",
    "                            morpho_map[tag][morpho_map[tag][val]] = val\n",
    "                if form not in vocab:\n",
    "                    vocab[form] = form\n",
    "                sentence.append((vocab[form], lemma, tags_list) )\n",
    "            elif len(sentence) > 0:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "    return sentences, morpho_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_tagged = \"../data/gikrya_train.txt\"\n",
    "path_to_write_morpho = \"../models/morpho.pickle\"\n",
    "sentences_full, morpho_map = read_gikrya(path_to_tagged)\n",
    "\n",
    "cat_order = sorted([key for key in morpho_map.keys()]) # sort categories\n",
    "pickle.dump((morpho_map, cat_order), open(path_to_write_morpho, 'wb')) # save dict and categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2)  стеммы и флексии \n",
    "\n",
    "Их нужно во-первых создать из тренировочных файлов\n",
    "\n",
    "во-вторых сделать word2vec модели из данных большого корпуса (кажется новостного)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_word(word, stemmer):\n",
    "    \"\"\"Отрезает флексию от слова. \n",
    "    Возвращает стем и флексию\"\"\"\n",
    "    flex = word[len(stemmer.stem(word)):]\n",
    "    if len(flex):\n",
    "        return word[:-len(flex)], flex\n",
    "    return word, \"empty\"\n",
    "\n",
    "def preproc_dataset(full_tag_sentences, stemmer):\n",
    "    \"\"\" \n",
    "    :param full_tag_sentences: array of sentence arrays. each sentence array has this structure:\n",
    "           [('рука', # token\n",
    "           'рука', # lemma\n",
    "           [('POS', 'NOUN'), # Gramms\n",
    "            ('Animacy', 'Inan'),\n",
    "            ('Case', 'Nom'),\n",
    "            ('Gender', 'Fem'),\n",
    "            ('Number', 'Sing')]), () ... () ]\n",
    "    :param stemmer: Snowball Stemmer\n",
    "    return: sentences - array of stem's sentences \n",
    "    flexes - array of flex's sentences\n",
    "    token_tags - array of tags's sentences\n",
    "    \"\"\"\n",
    "    sentences = []\n",
    "    flexes = []\n",
    "    token_tags = []\n",
    "    tokens = []\n",
    "    \n",
    "    for sent in full_tag_sentences:\n",
    "        temp_sent = []\n",
    "        temp_flexes = []\n",
    "        for token_info in sent:\n",
    "            token = token_info[0].lower()\n",
    "            tokens.append(token)\n",
    "            splits = split_word(token, stemmer) # tuple of (stem, flex)\n",
    "            temp_sent.append(splits[0])\n",
    "            temp_flexes.append(splits[1])\n",
    "            token_tags.append(token_info[2])  # надо бы переделать под стиль sentences или?          \n",
    "        sentences.append(temp_sent)\n",
    "        flexes.append(temp_flexes)    \n",
    "    return sentences, flexes, token_tags, tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['чья-т', 'рук', 'легл', 'ем', 'на', 'плеч', '.']]\n",
      "[['о', 'а', 'а', 'у', 'empty', 'о', 'empty']]\n",
      "[[('POS', 'DET'), ('Case', 'Nom'), ('Gender', 'Fem'), ('Number', 'Sing')]]\n",
      "['чья-то']\n"
     ]
    }
   ],
   "source": [
    "# Stemmer берем/ Обычный сноубол из nltk\n",
    "# и из тренировочного датасета делаем наборы флексий, стемм и т.д.\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "sentences, flexes, token_tags, tokens = preproc_dataset(sentences_full, stemmer)\n",
    "print(sentences[:1])\n",
    "print(flexes[:1])\n",
    "print(token_tags[:1])\n",
    "print(tokens[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Берем тексты уже готовые для эмбеддингов стемм и флексий. Кажется это были новости Лента"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    sentences = []\n",
    "    with open(path, 'r') as f:\n",
    "        for line in f:\n",
    "            sentences.append(line.strip().lower().split())\n",
    "    return sentences\n",
    "\n",
    "def get_tokens(sentences):\n",
    "    tokens = []\n",
    "    for sent in sentences:\n",
    "        for token in sent:\n",
    "            tokens.append(token)\n",
    "    return tokens\n",
    "\n",
    "stem_path = '../for_embedding/allTexts_stemmas.txt'\n",
    "flex_path = '../for_embedding/allTexts_flexias.txt'\n",
    "\n",
    "stemmas = read_corpus(stem_path)\n",
    "flexias = read_corpus(flex_path)\n",
    "\n",
    "# array of tokens\n",
    "stemmas = get_tokens(stemmas)\n",
    "flexias = get_tokens(flexias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из флексий и стеммов сделать словари, перекодировать их в id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514, 26)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UNDEFINED_TOKEN = \"undefined_token\"\n",
    "\n",
    "def build_vocab(sentences, min_freq=0, max_size=10000, undefined_id=0):\n",
    "    \"\"\" \n",
    "    Строит словарь из слов встертившихся более min_freq раз,\n",
    "    но размеров  не более max_size, в случае бОльшего количества токенов\n",
    "    отбрасываются менее частотные токены, undefined_id - id первого токена в словаре,\n",
    "    который будет называться \"undefined_token\"\n",
    "    \"\"\"\n",
    "    offset = undefined_id\n",
    "    token2id = {UNDEFINED_TOKEN: offset}\n",
    "    id2token = {offset: UNDEFINED_TOKEN}    \n",
    "    \n",
    "    counter = defaultdict(int)    \n",
    "    for sentence in sentences:\n",
    "        for token in sentence:\n",
    "            counter[token] += 1\n",
    "    sorted_tokens = [t_f[0]  for t_f in \n",
    "                     sorted([t_f for t_f in counter.items() if t_f[1] >= min_freq],\n",
    "                           key=lambda tf: -tf[1])]                     \n",
    "    \n",
    "    for token in sorted_tokens[:max_size - len(token2id)]:\n",
    "        offset += 1\n",
    "        token2id[token] = offset\n",
    "        id2token[offset] = token\n",
    "    return token2id, id2token \n",
    "\n",
    "# ну вообще-то скорее stem2id\n",
    "token2id, id2token = build_vocab(stemmas, \n",
    "                                 min_freq=1,\n",
    "                                 max_size=80000)\n",
    "flex2id, id2flex = build_vocab(flexias, \n",
    "                               min_freq=2, \n",
    "                               max_size=500)\n",
    "\n",
    "len(token2id), len(flex2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Тепероь обучить word2vec модели и сохранить их и словари:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514, 26)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def write_vecs(path, vecs_path, id2token, w2v_model):\n",
    "    # косяк с тем чтон undefined token не 0\n",
    "    vecs = np.zeros(shape=(len(token2id), w2v_model.vector_size))\n",
    "    with open(path, 'w') as f:\n",
    "        for tid in range(len(id2token)):\n",
    "            vecs[tid, :] = w2v_model[id2token[tid]]\n",
    "            f.write(id2token[tid])\n",
    "            f.write(\"\\n\")\n",
    "    np.save(vecs_path, vecs)\n",
    "\n",
    "# ???\n",
    "stem2stem = {}\n",
    "for stem in token2id.keys():\n",
    "    stem2stem[stem] = stem\n",
    "\n",
    "flex2flex = {}\n",
    "for flex in flex2id.keys():\n",
    "    flex2flex[flex] = flex\n",
    "\n",
    "new_sents = [[stem2stem.get(token, UNDEFINED_TOKEN) for token in sent] for sent in sentences]\n",
    "new_flexes = [[flex2flex.get(token, UNDEFINED_TOKEN) for token in sent] for sent in flexes]\n",
    "len(stem2stem), len(flex2flex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  сделать word2vec модели\n",
    "stem_model = Word2Vec(new_sents, size=200, sg=1, workers=5, iter=10, min_count=1)\n",
    "flex_model = Word2Vec(new_flexes, size=128, sg=1, workers=5, iter=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'н' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-1178861cedb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m write_vecs(os.path.join(prefix,\"stem2id\"),\n\u001b[1;32m      4\u001b[0m            \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stem_embeddings\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m            id2token, stem_model)\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m write_vecs(os.path.join(prefix, \"flex2id\"), \n",
      "\u001b[0;32m<ipython-input-12-a1194c2f1fb5>\u001b[0m in \u001b[0;36mwrite_vecs\u001b[0;34m(path, vecs_path, id2token, w2v_model)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mvecs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_model\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid2token\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/word2vec.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m   1279\u001b[0m         \u001b[0mRefer\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdocumentation\u001b[0m \u001b[0;32mfor\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgensim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m         \"\"\"\n\u001b[0;32m-> 1281\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1282\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1283\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, words)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[0;34m(self, word, use_norm)\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msyn0\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnegative\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"word 'н' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "# записать модели\n",
    "prefix = \"../models\"\n",
    "write_vecs(os.path.join(prefix,\"stem2id\"),\n",
    "           os.path.join(prefix, \"stem_embeddings\"),\n",
    "           id2token, stem_model)\n",
    "\n",
    "write_vecs(os.path.join(prefix, \"flex2id\"), \n",
    "           os.path.join(prefix, \"flex_embeddings\"),\n",
    "           id2flex, flex_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Само обучение модели (когда все эмбеддинги уже есть)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chars_to_features(tokens, char2id):\n",
    "    X = np.zeros(shape=(len(tokens), MAX_WORD_LENGTH), dtype=np.int)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        for chid in range(min(MAX_WORD_LENGTH, len(token))):\n",
    "            X[idx, -chid-1] = char2id.get(token[-chid-1], len(char2id))\n",
    "    return X\n",
    "\n",
    "def sentences_to_features(sentences, token2id, neighbors=3, undef_token=\"undefined_token\"):\n",
    "    arrays = [sentence_to_features(sent, token2id,  neighbors=neighbors,\n",
    "                                  undef_token=undef_token) for sent in sentences]\n",
    "    return np.vstack(arrays)\n",
    "\n",
    "def sentence_to_features(sentence, token2id, \n",
    "                         neighbors=3, undef_token=\"undefined_token\"):\n",
    "    \"\"\"\n",
    "    Делает из предложения \n",
    "    матрицу id слов, где  строка соответствует словам предложения:\n",
    "    в каждой строке состоит из neighbors id слов из левого контекста,\n",
    "    потом id слова, затем neighbors id слов правого контекста\n",
    "    0 - зарезерврован для паддинга, в словаре не должно быть слов с id 0\n",
    "    \"\"\"\n",
    "    X = np.ones(shape=(len(sentence), neighbors * 2 + 1), dtype=np.int) * len(token2id)\n",
    "    id_seq = np.zeros(shape=(len(sentence) + 2*neighbors,), dtype=np.int)\n",
    "    for idx, token in enumerate(sentence):\n",
    "        num = token2id.get(token, token2id[undef_token])\n",
    "        id_seq[idx+neighbors] = num\n",
    "    for idx in range(len(sentence)):\n",
    "        X[idx, :] = id_seq[idx:idx + X.shape[1]]\n",
    "    return X\n",
    "\n",
    "def tagsets_to_one_hot(tagsets, morpho_map, cat_order):    \n",
    "    # при частых запусках не оптимально так:\n",
    "    # cats = set([cat for cat, val in tag2id.keys()])\n",
    "    y = [np.zeros(shape=(len(tagsets), len(morpho_map[cat]) // 2), dtype=np.int) \n",
    "         for cat in cat_order]\n",
    "    \n",
    "    for one_hot in y:\n",
    "        one_hot[:, 0] = 1       \n",
    "        \n",
    "    for idx, tagset in enumerate(tagsets):                    \n",
    "        for cat, tag in tagset:\n",
    "            # не очень эффективно индекс искать постоянно\n",
    "            \n",
    "            cat_id = cat_order.index(cat)    \n",
    "            if cat_id >= 0:\n",
    "                y[cat_id][idx, 0] = 0\n",
    "                y[cat_id][idx, morpho_map[cat].get(tag, 0)] = 1            \n",
    "    return y\n",
    "\n",
    "def make_dataset(path, stemmer,\n",
    "                      morpho_map, cat_order, undef_token,\n",
    "                      token2id, flex2id, char2id,\n",
    "                      neighbors=3):\n",
    "    full_tag_sentences, _ = read_gikrya(path)    # up\n",
    "    print('Done')\n",
    "    sentences, flexes, token_tags, tokens = preproc_dataset(full_tag_sentences, stemmer) # up \n",
    "    print('Preprocessed')\n",
    "    X_stem = sentences_to_features(sentences, token2id,\n",
    "                                   neighbors=neighbors, undef_token=undef_token)\n",
    "    print(\"Stems are ready\")\n",
    "    X_flex = sentences_to_features(flexes, flex2id, \n",
    "                                   neighbors=neighbors,\n",
    "                                   undef_token=undef_token)\n",
    "    print(\"Flexes are ready\")\n",
    "    X = chars_to_features(tokens, char2id)\n",
    "    y = tagsets_to_one_hot(token_tags, morpho_map, cat_order)\n",
    "    return X_stem, X_flex, X, y, sentences, flexes, token_tags, full_tag_sentences\n",
    "\n",
    "\n",
    "def read_embeddings(vocab_path, emb_path):\n",
    "    \"\"\"Считываем файлы уже гтовых эмбеддингов\n",
    "     return: \n",
    "     data2id - dictionary {char: id}\n",
    "     vectors - \n",
    "     tokens - только undefined_token тут важен\n",
    "     \"\"\"\n",
    "    token2id = {}\n",
    "    tokens = open(vocab_path, \"r\").read().strip().split(\"\\n\")    \n",
    "    for i, ch in enumerate(tokens):\n",
    "        token2id[ch] = i\n",
    "    vecs = np.load(emb_path)\n",
    "    rnd_vec = np.random.uniform(size=vecs.shape[1]) \n",
    "    return token2id, np.vstack((vecs, rnd_vec / np.linalg.norm(rnd_vec))),  tokens[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "Preprocessed\n",
      "Stems are ready\n",
      "Flexes are ready\n",
      "Done\n",
      "Preprocessed\n",
      "Stems are ready\n",
      "Flexes are ready\n"
     ]
    }
   ],
   "source": [
    "# загружаем уже готовый pickle со словарем\n",
    "morpho_map, cat_order =  pickle.load(open(\"../models/morpho.pickle\", 'rb'))\n",
    "# загружаем уже готовые эмбеддинги стемов, флексий и лемм \n",
    "token2id, token_vecs, undef_token = read_embeddings(os.path.join(prefix, \"stem2id\"),\n",
    "                                      os.path.join(prefix, \"stem_embeddings.npy\"))\n",
    "\n",
    "flex2id, flex_vecs, _ = read_embeddings(os.path.join(prefix, \"flex2id\"),\n",
    "                                     os.path.join(prefix, \"flex_embeddings.npy\"))\n",
    "\n",
    "char2id = {}\n",
    "chars = open(\"char2id\", \"r\").read().strip().split(\"\\n\")\n",
    "for i, ch in enumerate(chars):\n",
    "    char2id[ch] = i\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "neighbors = 3\n",
    "gikrya_path = \"../data/gikrya_train.txt\"\n",
    "X_stem_train, X_flex_train, X_train, y_train, sentences, flexes,\\\n",
    "token_tags, full_tag_sentences = make_dataset(gikrya_path, stemmer, \n",
    "                                            morpho_map, cat_order, undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)\n",
    "\n",
    "gikrya_test = \"../data/gikrya_test.txt\"\n",
    "X_stem_test, X_flex_test, X_test, y_test, sentences, flexes,\\\n",
    "token_tags, full_tag_test = make_dataset(gikrya_test, stemmer, \n",
    "                                            morpho_map, cat_order,undef_token, \n",
    "                                            token2id, flex2id, char2id, neighbors=neighbors)\n",
    "\n",
    "# как-то это совмещается\n",
    "X_stem, X_flex, X = np.vstack((X_stem_train, X_stem_test)),\\\n",
    "                                   np.vstack((X_flex_train, X_flex_test)),\\\n",
    "                                   np.vstack((X_train, X_test))\n",
    "\n",
    "y = [np.vstack((train, test)) for train, test in zip(y_train, y_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=14)`\n"
     ]
    }
   ],
   "source": [
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "\n",
    "char_in = Input(shape=(X.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,\n",
    "                           output_dim=char2vec_dim)       \n",
    "\n",
    "encoded_char = Bidirectional(LSTM(char_hidden,\n",
    "                                   dropout_U=0.2, \n",
    "                                   dropout_W=0.2))(char_embedding(char_in))    \n",
    "\n",
    "\n",
    "stem_in = Input(shape=(X_stem_train.shape[1],))\n",
    "stem_embedding = Embedding(input_dim=token_vecs.shape[0],\n",
    "                           output_dim=token_vecs.shape[1],\n",
    "                           weights=[token_vecs])       \n",
    "\n",
    "encoded_stem = LSTM(stem_hidden,\n",
    "                    dropout_U=0.2, \n",
    "                    dropout_W=0.2)(stem_embedding(stem_in))\n",
    "\n",
    "merged = keras.layers.merge([encoded_char, encoded_stem], mode='concat')\n",
    "# merged = keras.layers.merge([encoded_char, encoded_flex], mode='concat')\n",
    "pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1], \n",
    "            activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:21: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1010117 samples, validate on 76031 samples\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-590c4e3ffb8f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           verbose=2)\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m   1428\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1429\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1430\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1431\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m   1077\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2267\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2269\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# компилим модель\n",
    "model = Model([char_in, stem_in], pos_predict)\n",
    "model.compile(loss='categorical_crossentropy', \n",
    "              optimizer='rmsprop',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "test_fraction = 0.07\n",
    "shuffled_indicies = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffled_indicies)\n",
    "split_index = int(X.shape[0] * (1 - test_fraction))\n",
    "train = shuffled_indicies[:split_index]\n",
    "test = shuffled_indicies[split_index:]\n",
    "\n",
    "# Запустить модель двойная со стемами\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "model.fit([X[train], X_stem[train]], y[cat_order.index('POS')][train],          \n",
    "          validation_data=([X[test], X_stem[test]], y[cat_order.index('POS')][test]),          \n",
    "          batch_size=batch_size,           \n",
    "          nb_epoch=epochs, \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Варианты моделей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Было несколько её модификаций. Просто со стемами была лучше\n",
    "результаты: лучший за 10 эпох 0.9874"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(128, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:26: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:29: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=14)`\n"
     ]
    }
   ],
   "source": [
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,\n",
    "                           output_dim=char2vec_dim)       \n",
    "\n",
    "encoded_char = Bidirectional(LSTM(char_hidden,\n",
    "                                   dropout_U=0.2, \n",
    "                                   dropout_W=0.2))(char_embedding(char_in))    \n",
    "\n",
    "\n",
    "stem_in = Input(shape=(X_stem_train.shape[1],))\n",
    "stem_embedding = Embedding(input_dim=token_vecs.shape[0],\n",
    "                           output_dim=token_vecs.shape[1],\n",
    "                           weights=[token_vecs])       \n",
    "\n",
    "encoded_stem = LSTM(stem_hidden,\n",
    "                    dropout_U=0.2, \n",
    "                    dropout_W=0.2)(stem_embedding(stem_in))\n",
    "\n",
    "merged = keras.layers.merge([encoded_char, encoded_stem], mode='concat')\n",
    "pos_predict = Dense(output_dim=y_train[cat_order.index('POS')].shape[1], \n",
    "            activation='softmax')(merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Но были и другие версии:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:14: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  \n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:17: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=14)`\n"
     ]
    }
   ],
   "source": [
    "# это для примера, не запускать!\n",
    "# 1) Простая модификация. результаты: лучший за 20 эпох 0.976\n",
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1,\n",
    "                           output_dim=char2vec_dim)\n",
    "\n",
    "encoded_char = Bidirectional(LSTM(char_hidden, \n",
    "                                  dropout_U=0.2, \n",
    "                                  dropout_W=0.2))(char_embedding(char_in))\n",
    "\n",
    "pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1], \n",
    "                    activation='softmax',)(encoded_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, dropout=0.2, recurrent_dropout=0.2)`\n",
      "  if sys.path[0] == '':\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:19: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, dropout=0.2, recurrent_dropout=0.2)`\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:22: UserWarning: The `merge` function is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "/usr/local/lib/python3.6/site-packages/keras/legacy/layers.py:458: UserWarning: The `Merge` layer is deprecated and will be removed after 08/2017. Use instead layers from `keras.layers.merge`, e.g. `add`, `concatenate`, etc.\n",
      "  name=name)\n",
      "/usr/local/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=14)`\n"
     ]
    }
   ],
   "source": [
    "# пример с флексиями! не запускать\n",
    "# результаты: лучший за 10 эпох 0.9773\n",
    "token_hidden = 128\n",
    "flex_hidden = 64\n",
    "char2vec_dim = 42\n",
    "char_hidden = 512\n",
    "stem_hidden = 128\n",
    "flex_hidden = 128\n",
    "char_in = Input(shape=(X_train.shape[1],))\n",
    "char_embedding = Embedding(input_dim=len(char2id) + 1, output_dim=char2vec_dim)\n",
    "\n",
    "encoded_char = Bidirectional(LSTM(char_hidden, dropout_U=0.2, dropout_W=0.2))(char_embedding(char_in))\n",
    "\n",
    "flex_in = Input(shape=(X_flex_train.shape[1],))\n",
    "flex_embedding = Embedding(input_dim=flex_vecs.shape[0], \n",
    "                           output_dim=flex_vecs.shape[1], weights=[flex_vecs])\n",
    "encoded_flex = LSTM(char_hidden, \n",
    "                    dropout_U=0.2, \n",
    "                    dropout_W=0.2)(flex_embedding(flex_in))\n",
    "\n",
    "merged = keras.layers.merge([encoded_char, encoded_flex], \n",
    "                            mode='concat') \n",
    "\n",
    "pos_predict = Dense(output_dim=y[cat_order.index('POS')].shape[1], activation='softmax')(merged)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# готовим всё для fit модели\n",
    "test_fraction = 0.07\n",
    "shuffled_indicies = np.arange(X.shape[0])\n",
    "np.random.shuffle(shuffled_indicies)\n",
    "split_index = int(X.shape[0] * (1 - test_fraction))\n",
    "train = shuffled_indicies[:split_index]\n",
    "test = shuffled_indicies[split_index:]\n",
    "\n",
    "# fit для варианта с флексиями:\n",
    "batch_size = 256\n",
    "epochs = 10\n",
    "model.fit([X[train], X_flex[train]], y[cat_order.index('POS')][train],          \n",
    "          validation_data=([X[test], X_flex[test]], y[cat_order.index('POS')][test]),          \n",
    "          batch_size=batch_size,           \n",
    "          nb_epoch=epochs, \n",
    "          verbose=2)\n",
    "\n",
    "# fit кажется для простого варианта\n",
    "# batch_size = 256\n",
    "# epochs = 10\n",
    "# model.fit(X[train], y[cat_order.index('POS')][train],          \n",
    "#          validation_data=(X[test], y[cat_order.index('POS')][test]),          \n",
    "#          batch_size=batch_size, nb_epoch=epochs, \n",
    "#          verbose=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
