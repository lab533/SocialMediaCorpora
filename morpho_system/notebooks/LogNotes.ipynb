{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# A PLAN:\n",
    "Итак пока идея такая: берем [mnist_hierarchical_rnn.py](https://github.com/fchollet/keras/blob/master/examples/mnist_hierarchical_rnn.py) \n",
    "и юзаем\n",
    "На вход подаем вектор стемма и отдельно вектор флексии, думаю для этого можно использовать\n",
    "embedding layers\n",
    "\n",
    "TO DO:\n",
    "- ~~запилить хотя бы просто CountVectorizer для формирования интов на входы\n",
    "embeddings~~ \n",
    "\n",
    "> запилил build_vocab\n",
    "\n",
    "- ~~запилить преобразования целевых тэгов в onehot или еще как~~\n",
    "- ~~поднять сеть HierarchicalRNN со стеммаим на входе  и только POS на выходе~~\n",
    "- ~~допилить туда флексии~~\n",
    "- ~~попробовать поменять LSTM на SimpleRNN~~\n",
    "\n",
    "> учиться чуть быстрее(примерно в 4 раза) результаты чуть медленнее сходятся (примерно в 2 раз)..\n",
    "\n",
    "- ~~попробовать GRU ?~~\n",
    "- ~~допилить классификаторы для остальных тэггов~~\n",
    "- ~~make ud format output for eval task results~~\n",
    "- ~~прописать коллбэки для earl\n",
    "y_stopping и сохранения лучшей модели~~\n",
    "- ~~попробовать контекст побольше?~~ с 5 соседями с каждой стороны не взлетело\n",
    "- ~~настройка видюхи геракла~~\n",
    "- ~~сделать ноутбук для построения вордвеков~~\n",
    "- ~~переделать схему инциализации словарей стеммов и окончаний~~\n",
    "- ~~добавить отдельным входом вордвек стема и вордвек окончания~~\n",
    "- исправить баг с нумерацией token2id ???\n",
    "- добавить в построения w2v тестовую и учебную выборки\n",
    "- сделать embeddings trainable=False\n",
    "- попробовать опенкорпоровскую морфологию из словаря, т.е. слова без контекста\n",
    "- ~~[опционально] запилить посимвольный перевод слова через RNN в вектор~~ сделал\n",
    "отдельный генератор текста по символам и потом пытался через него слова прогонять\n",
    "- запилить сетку работающую со словом как с набором символов, для только POS\n",
    "- сделать скрипт обучения модели, вход: [коллекция, директория для модели, [тест для валидации]]\n",
    "выход: сохраненная модель\n",
    "- сделать скрипт разметки вход: [коллекция, модель] выход: ud разметка\n",
    "- проверить UNDEFINED\n",
    "- ???\n",
    "- profit\n",
    "\n",
    "#### Литература:\n",
    "\n",
    "1. [A Hierarchical Neural Autoencoder for Paragraphs and Documents](https://arxiv.org/pdf/1506.01057.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "\n",
    "### SOME RESULTS:\n",
    "\n",
    "#### первые шаги:\n",
    "gikrya_train, вход: флексии + стеммы, выход: POS, словарь токенов max_size=2000, словарь флексий max_size=500, val_split=0.1, промежуточный результат:\n",
    "\n",
    ">Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    ">Epoch 1/5: 2436s - loss: 0.3060 - acc: 0.9024 - val_loss: 0.1234 - val_acc: 0.9630\n",
    "\n",
    ">Epoch 2/5: 2407s - loss: 0.1219 - acc: 0.9637 - val_loss: 0.1003 - val_acc: 0.9687\n",
    "\n",
    ">Epoch 3/5: 2390s - loss: 0.1023 - acc: 0.9694 - val_loss: 0.0910 - val_acc: 0.9727\n",
    "\n",
    ">Epoch 4/5: 2401s - loss: 0.0938 - acc: 0.9724 - val_loss: 0.0852 - val_acc: 0.9752\n",
    "\n",
    "> Epoch 5/5: 2397s - loss: 0.0888 - acc: 0.9744 - val_loss: 0.0853 - val_acc: 0.9764\n",
    "\n",
    "+\n",
    "\n",
    ">Epoch 6: 2397s - loss: 0.0866 - acc: 0.9754 - val_loss: 0.0901 - val_acc: 0.9757\n",
    "\n",
    "#### Замена LSTM на Simple RNN\n",
    "> Epoch 2: 663s - loss: 0.1494 - acc: 0.9551 - val_loss: 0.1181 - val_acc: 0.9645\n",
    "\n",
    "> Epoch 3: 664s - loss: 0.1340 - acc: 0.9598 - val_loss: 0.1116 - val_acc: 0.9674\n",
    "\n",
    "+\n",
    "\n",
    "> Epoch 4: 663s - loss: 0.1266 - acc: 0.9624 - val_loss: 0.1075 - val_acc: 0.9672\n",
    "\n",
    "\n",
    "#### Замена GRU\n",
    "\n",
    ">Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    ">Epoch 1/2: 1922s - loss: 0.2765 - acc: 0.9138 - val_loss: 0.1233 - val_acc: 0.9627\n",
    "\n",
    ">Epoch 2/2: 1912s - loss: 0.1249 - acc: 0.9627 - val_loss: 0.0975 - val_acc: 0.9706\n",
    "\n",
    "#### GRU all outputs:\n",
    "\n",
    "Train on 734295 samples, validate on 81589 samples\n",
    "\n",
    "> Epoch 3: \n",
    "2899s - loss: 0.7679 - dense_12_loss: 0.0605 - dense_13_loss: 0.1723 - dense_14_loss: 0.0552 - dense_15_loss: 0.0025 - dense_16_loss: 0.1316 - dense_17_loss: 0.0200 - dense_18_loss: 0.1102 - dense_19_loss: 0.1123 - dense_20_loss: 0.0242 - dense_21_loss: 0.0242 - dense_22_loss: 0.0117 - dense_23_loss: 0.0228 - dense_24_loss: 0.0203 - dense_12_acc: 0.9812 - dense_13_acc: 0.9423 - dense_14_acc: 0.9819 - dense_15_acc: 0.9993 - dense_16_acc: 0.9582 - dense_17_acc: 0.9940 - dense_18_acc: 0.9623 - dense_19_acc: 0.9659 - dense_20_acc: 0.9926 - dense_21_acc: 0.9925 - dense_22_acc: 0.9964 - dense_23_acc: 0.9932 - dense_24_acc: 0.9938 - val_loss: 0.6724 - val_dense_12_loss: 0.0527 - val_dense_13_loss: 0.1537 - val_dense_14_loss: 0.0475 - val_dense_15_loss: 0.0019 - val_dense_16_loss: 0.1114 - val_dense_17_loss: 0.0184 - val_dense_18_loss: 0.1004 - val_dense_19_loss: 0.0976 - val_dense_20_loss: 0.0211 - val_dense_21_loss: 0.0209 - val_dense_22_loss: 0.0090 - val_dense_23_loss: 0.0206 - val_dense_24_loss: 0.0173 - val_dense_12_acc: 0.9832 - val_dense_13_acc: 0.9469 - val_dense_14_acc: 0.9844 - val_dense_15_acc: 0.9995 - val_dense_16_acc: 0.9650 - val_dense_17_acc: 0.9945 - val_dense_18_acc: 0.9670 - val_dense_19_acc: 0.9697 - val_dense_20_acc: 0.9943 - val_dense_21_acc: 0.9937 - val_dense_22_acc: 0.9974 - val_dense_23_acc: 0.9941 - val_dense_24_acc: 0.9948\n",
    "\n",
    ">Epoch 4:\n",
    "2892s - loss: 0.7051 - dense_12_loss: 0.0549 - dense_13_loss: 0.1598 - dense_14_loss: 0.0513 - dense_15_loss: 0.0022 - dense_16_loss: 0.1193 - dense_17_loss: 0.0182 - dense_18_loss: 0.1010 - dense_19_loss: 0.1039 - dense_20_loss: 0.0221 - dense_21_loss: 0.0222 - dense_22_loss: 0.0109 - dense_23_loss: 0.0208 - dense_24_loss: 0.0184 - dense_12_acc: 0.9832 - dense_13_acc: 0.9467 - dense_14_acc: 0.9833 - dense_15_acc: 0.9994 - dense_16_acc: 0.9626 - dense_17_acc: 0.9946 - dense_18_acc: 0.9658 - dense_19_acc: 0.9685 - dense_20_acc: 0.9933 - dense_21_acc: 0.9933 - dense_22_acc: 0.9967 - dense_23_acc: 0.9940 - dense_24_acc: 0.9946 - val_loss: 0.6327 - val_dense_12_loss: 0.0487 - val_dense_13_loss: 0.1437 - val_dense_14_loss: 0.0461 - val_dense_15_loss: 0.0016 - val_dense_16_loss: 0.1043 - val_dense_17_loss: 0.0196 - val_dense_18_loss: 0.0904 - val_dense_19_loss: 0.0913 - val_dense_20_loss: 0.0180 - val_dense_21_loss: 0.0209 - val_dense_22_loss: 0.0085 - val_dense_23_loss: 0.0214 - val_dense_24_loss: 0.0182 - val_dense_12_acc: 0.9846 - val_dense_13_acc: 0.9505 - val_dense_14_acc: 0.9841 - val_dense_15_acc: 0.9996 - val_dense_16_acc: 0.9680 - val_dense_17_acc: 0.9944 - val_dense_18_acc: 0.9700 - val_dense_19_acc: 0.9720 - val_dense_20_acc: 0.9951 - val_dense_21_acc: 0.9938 - val_dense_22_acc: 0.9974 - val_dense_23_acc: 0.9941 - val_dense_24_acc: 0.9947\n",
    "\n",
    "\n",
    "eval на gikrya_test:\n",
    "\n",
    "> 149081 меток из 171550, точность 86.90%\n",
    "\n",
    "> 8454 предложений из 20787, точность 40.67%\n",
    "\n",
    "#### LSTM \n",
    "eval на gikrya_test:\n",
    "\n",
    "> 149674 меток из 171550, точность 87.25%\n",
    "\n",
    "> 8751 предложений из 20787, точность 42.10%\n",
    "\n",
    "\n",
    "#### LSTM + добавил на выходе промежуточный dense слой  на 150 нейронов с активацие relu\n",
    "eval на gikrya_test:\n",
    "\n",
    "> 159084 меток из 171550, точность 92.73%\n",
    "\n",
    "> 12657 предложений из 20787, точность 60.89%\n",
    "\n",
    "#### LSTM + добавил на выходе промежуточный dense слой  на 150 нейронов с активацие relu + отдельно лемма + окончание\n",
    "\n",
    "> 160055 меток из 171550, точность 93.30%\n",
    "> 13234 предложений из 20787, точность 63.66%\n",
    "\n",
    "На тесте VK\n",
    "> 3225 меток из 3877, точность 83.18%\n",
    "\n",
    "> 217 предложений из 568, точность 38.20%\n",
    "\n",
    "#### LSTM + добавил на выходе промежуточный dense слой  на 200 нейронов с активацие relu + слово посимвольно (символы в обратном порядке), на всем гикря за исключение 0.05 части выборки на лосс\n",
    "\n",
    "На тесте VK\n",
    "> \n",
    "\n",
    "> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### POS only tagger\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
